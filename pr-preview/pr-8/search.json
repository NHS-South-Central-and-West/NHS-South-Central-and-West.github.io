[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Code Club!",
    "section": "",
    "text": "Brought to you by the Specialist Analytics Team\nYou can find our Teams channel here\n\nMission statement\nHere at Code Club our main aim is to bring more advanced analytical skills to the organisation, and we believe that the best medium for this is interpretive dance coding. We want to foster a programming ecosystem at SCW so that we can all learn from each other and improve the way we do things together. The Code Club syllabus has been designed to support people with little-to-no coding experience with extending their analytical skills and driving efficiency through the automation of day-to-day tasks. We believe that these are indispensable skills in an ever-changing NHS landscape, where delivering high quality insights efficiently, economically and systematically is more essential than ever. This won’t merely be a series of lectures; it will be a place to spark ideas and nurture collaboration.\n\n\nWhat to expect\nSessions will be an hour long, held once per fortnight- 2:00 PM on a Thursday. Each hour will cover a topic from the syllabus, followed by a time for a Q & A with your Code Club hosts, whether that’s on the theme of the day or troubleshooting any coding issues you have run into. Aside from that, Code Clubbers are welcome to submit requests to cover a topic of interest -or indeed be a guest speaker themselves! -and we will find the best week for it to fit in around the syllabus. Most of the syllabus will be delivered with Python as the main programming language, but that isn’t to say that other languages won’t feature and won’t be welcome. If you want to get an idea of what we will be covering and see if it is right for you, head to the Schedule page.\nCredit for the Code Club logo goes to our erstwhile colleague Jeffrey Morgan-Harrisskitt",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html",
    "href": "sessions/01-onboarding/index.html",
    "title": "Python Onboarding",
    "section": "",
    "text": "This is the first session of Code Club’s relaunch. It focuses on giving users all the tools they need to get started using Python and demonstrates the setup for a typical Python project.",
    "crumbs": [
      "Sessions",
      "Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html#session-slides",
    "href": "sessions/01-onboarding/index.html#session-slides",
    "title": "Python Onboarding",
    "section": "Session Slides",
    "text": "Session Slides\nUse the left ⬅️ and right ➡️ arrow keys to navigate through the slides below. To view in a separate tab/window, follow this link.",
    "crumbs": [
      "Sessions",
      "Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html#the-tools-you-will-need",
    "href": "sessions/01-onboarding/index.html#the-tools-you-will-need",
    "title": "Python Onboarding",
    "section": "The Tools You Will Need",
    "text": "The Tools You Will Need\nWhile this course focuses on Python, we will use several other tools throughout.\n\nLanguage: Python\nDependency Management & Virtual Environments: uv\nVersion Control: Git, GitHub Desktop\nIDE: VS Code/Jupyter Notebooks (or your preferred IDE)\n\nYou can install all the tools you’ll need by running the following one-liner run in PowerShell:\nwinget install astral-sh.uv Microsoft.VisualStudioCode github-desktop\n\nPython\nPython is an all-purpose programming language that is one of, if not the most popular, in the world1 and is widely used in almost every industry. Its popularity is owed to its flexibility as a language that can be used to achieve nearly any job. It is often referred to as the second-best tool for every job. Specialist languages might be better for specific tasks (for example, R for statistics), but Python is good at everything.\nIt is a strong choice for data science and analytics, being one of the best languages for data wrangling, data visualisation, statistics, and machine learning. It is also well-suited to web development, scientific computing, and automation.\n\n\nDependency Management\nOne of Python’s greatest weaknesses is dependency management. Despite its many strengths, there is no escaping the dependency hell that every Python user faces.\nDependency management refers to the process of tracking and managing all of the packages (dependencies) a project needs to run. It is a consideration in any programming language. It ensures:\n\nThe right packages are installed.\nThe correct versions are used.\nConflicts between packages are avoided.\n\nThere are many reasons that Python handles dependency management so poorly, but there are some tools that make this a little easier on users. We are using uv for dependency management. It is relatively new, but it is quickly becoming the consensus tool for dependency management in Python because it makes the process about as painless as it can be without moving to a different language entirely.\n\nVirtual Environments\nVirtual environments are a component of dependency management. Dependency management becomes much messier when you have many Python projects, each using their own packages, some overlapping and some requiring specific versions, either for compatibility or functionality reasons. Reducing some of this friction by isolating each project in its own virtual environment, like each project is walled off from all other projects, makes dependency management a little easier. Virtual environments allow you to manage dependencies for a specific project without the state of those dependencies affecting other projects or your wider system.\nVirtual environments help by:\n\nKeeping dependencies separate for each project.\nAvoiding version conflicts between projects.\nMaking dependency management more predictable and reproducible.\n\nWe will use uv to manage all dependencies, virtual environments, and even versions of Python.\n\n\n\nVersion Control\nVersion control is the practice of tracking and managing changes to code or files over time, allowing you to:\n\nRevert to earlier versions if needed.\nCollaborate with others on the same project easily.\nMaintain a history of changes.\n\nWe are using Git, a version control system, to host our work and GitHub Desktop to manage version control locally.\nVersion control and Git are topics entirely in their own right, and covering them in detail is out of the scope of this session. We hope to cover version control in a future session, but right now, you just need to be able to access materials for these sessions. You can find the materials in the Code Club repository.\nIf you have downloaded GitHub Desktop, the easiest way to access these materials and keep up-to-date is by cloning the Code Club repository (go to File, then Clone Repository, select URL, and paste the Code Club repository link in the URL field). You can then ensure that the materials you are using are the most current by clicking the Fetch Origin button in GitHub Desktop, which grabs the changes we’ve made from the central repository on GitHub.\n\n\nIDE\nIDEs (Integrated Development Environments) are software that simplifies programming and development by combining many of the most common tasks and helpful features for programming into a single tool. These typically include a code editor, debugging functionality, build tools, and features like syntax highlighting and code completion. When you start your code journey, you might not need all these tools, and fully-featured IDEs can be overwhelming. But as you become more comfortable with programming, all these different features will become very valuable.\nSome common IDEs that are used for Python include:\n\nVS Code\nPyCharm\nVim\nJupyter Notebooks/JupyterLab\nPositron\n\nWe will use VS Code or Jupyter Notebooks (which is not exactly an IDE but is similar).",
    "crumbs": [
      "Sessions",
      "Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html#project-setup",
    "href": "sessions/01-onboarding/index.html#project-setup",
    "title": "Python Onboarding",
    "section": "Project Setup",
    "text": "Project Setup\nEvery new Python project should start with using uv to set up a virtual environment for the project to keep everything organised and reduce the risk of finding yourself in dependency hell.\nThe entire project setup process can be handled in the command line. We will use PowerShell for consistency.\nWhen you open a PowerShell window, it should open in your C drive (e.g.,  C:\\Users\\user.name). If it does not, run cd ~, and it should return to your home directory.\nWe will create a new uv project in the home directory2 using the command uv init. The new project will contain everything we need, including a Python installation, a virtual environment, and the necessary project files for tracking and managing any packages installed in the virtual environment. To set up a new project called test-project, use the following command:\nuv init test-project\nHaving created this new directory, navigate to it using cd test-project. You can check the files in a directory using the command ls. If you run this command, you will see three files in the project directory (hello.py, pyproject.toml, and README.md). The project doesn’t yet have a Python installation or a virtual environment, but this will be added when we add external Python packages.\nYou can install Python packages using the command uv add. We can add some common Python packages that we will use in most projects (pandas, numpy, seaborn, and ipykernel3) using the following command:\nuv add pandas numpy seaborn ipykernel\nThe output from this command will reference the Python installation used and the creation of a virtual environment directory .venv. Now, if you run ls, you will see two new items in the directory, uv.lock and .venv.\nYour Python project is now set up, and you are ready to start writing some code. You can open VS Code from your PowerShell window by running code ..\nFor more information about creating and managing projects using uv, check out the uv documentation.\n\nOpening your project in VS Code\nTo open your newly-created uv project in VS Code, launch the application and click File &gt; Open Folder.... You’ll want to make sure you select the root level of your project. Once you’ve opened the folder, the file navigation pane in VS Code should display the files that uv has created, including a main.py example file. Click on this to open it.\nOnce VS Code realises you’ve opened a folder with Python code and a virtual environment, it should do the following:\n\nSuggest you install the Python extension (and, once you’ve created a Jupyter notebook, the Jupyter one) offered by Microsoft - go ahead and do this. If this doesn’t happen, you can install extensions manually from the Extensions pane on the left-hand side.\nSelect the uv-created .venv as the python Environment we’re going to use to actually run our code. If this doesn’t happen, press ctrl-shift-P, type “python environment” to find the Python - Create Environment... option, hit enter, choose “Venv” and proceed to “Use Existing”.\n\nIf all has gone well, you should be able to hit the “play” icon in the top right to execute main.py. The Terminal pane should open up below and display something like Hello from (your-project-name)!.",
    "crumbs": [
      "Sessions",
      "Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html#footnotes",
    "href": "sessions/01-onboarding/index.html#footnotes",
    "title": "Python Onboarding",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlthough there are several ways to measure language popularity, the PYPL Index, HackerRank’s Developer Skills Report, and IEEE Spectrum all rank Python as the most popular language in the world, while Stack Overflow’s Developer Survey places Python third behind JavaScript and HTML/CSS.↩︎\nWe recommend using the C drive for all Python projects, especially if using version control. Storing projects like these on One Drive will create many unnecessary issues.↩︎\nStrictly speaking, we should install ipykernel as a development dependency (a dependency that is needed for any development but not when the project is put into production). In this case, we would add it by running uv add --dev ipykernel. However, in this case, it is simpler to just add it as a regular dependency, and it doesn’t harm.↩︎",
    "crumbs": [
      "Sessions",
      "Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/10-comparing-samples/index.html",
    "href": "sessions/10-comparing-samples/index.html",
    "title": "Comparison - Thief of Joy?",
    "section": "",
    "text": "This notebook walks through the process of comparing samples, demonstrating why comparisons matter and how we go about them, applying these ideas using data on fatal car crashes in the U.S.\nWe’ll cover: - Why we compare groups in data analysis - How samples differ from populations - How to visualize and interpret group differences - How to assess whether a difference is meaningful (statistically)",
    "crumbs": [
      "Sessions",
      "Regression - Comparing Samples"
    ]
  },
  {
    "objectID": "sessions/10-comparing-samples/index.html#why-compare",
    "href": "sessions/10-comparing-samples/index.html#why-compare",
    "title": "Comparison - Thief of Joy?",
    "section": "Why Compare?",
    "text": "Why Compare?\nMotivating example: Imagine an online store tests two versions of its homepage: Version A and Version B. After a week, it finds Version B users spent more money on average. - Is that difference real, or just random chance? - Could we expect the same pattern next week?\nTakeaway: Comparing samples helps us decide whether patterns in our data are meaningful or simply random variation. It’s a core building block before modelling relationships.\nQuestions:\n\nCan you think of any healthcare-related comparisons similar to the motivating example?\nHow do you currently decide whether the difference you observe in your data is real or occurred by chance?\nWhy is it important to know if differences observed in data occurred by chance?\n\n\nPopulation vs. Sample\n\n\n\nSource: Martijn Wieling\n\n\nKey concepts:\n\nPopulation: the full group we’re interested in (e.g. all customers, all days from 1992–2016)\nSample: a subset we actually observe or focus on (e.g. one week’s users, a set of 4/20 days)\n\nIf we had access to the population, comparisons would be straightforward. But we usually don’t—so we have to take a sample of the population and make inferences about the population from our sample. That means dealing with uncertainty, variation, and potential bias.\nTo compare groups responsibly, we need to consider how sampling affects what we observe, and how it may limit our comparisons. The sample is a small snapshot of the population, and there are lots of reasons why a sample might not be representative of the wider population.\n\nimport numpy as np\nimport random\n\n# simulate drawing 10 cards from a standard deck\ndeck = list(range(1, 14)) * 4\n\n# draw two random samples of ten cards\nsample1 = random.sample(deck, 5)\nsample2 = random.sample(deck, 5)\n\n# compute sample means\nmean1 = np.mean(sample1)\nmean2 = np.mean(sample2)\n\n# compute population mean\npopulation_mean = np.mean(deck)\n\nprint(\"Sample means:\", mean1, mean2)\nprint(\"Population mean:\", population_mean)\n\nSample means: 7.0 6.4\nPopulation mean: 7.0\n\n\nQuestion: Why are the sample means different? And why are they different from the population mean?\nTakeaway: Even if the process is fair, individual samples vary. This is a core challenge of inference.\nWe rely on well-designed comparisons to manage these uncertainties, using statistical tools that help us determine whether sample-level observations likely reflect real population-level differences.",
    "crumbs": [
      "Sessions",
      "Regression - Comparing Samples"
    ]
  },
  {
    "objectID": "sessions/10-comparing-samples/index.html#comparing-car-crash-fatalities---high-or-not",
    "href": "sessions/10-comparing-samples/index.html#comparing-car-crash-fatalities---high-or-not",
    "title": "Comparison - Thief of Joy?",
    "section": "Comparing Car Crash Fatalities - High or Not?",
    "text": "Comparing Car Crash Fatalities - High or Not?\nNow we can apply this logic to a real-world dataset. We will use a dataset that records the daily count of fatal U.S. car crashes from 1992–2016, taken from a study into the effects of the annual cannabis holiday, 4/20, on fatal car accidents. Previous research has concluded that fatalities are higher on 4/20, suggesting that the holiday is the cause of the increase.\nWe are using a dataset, provided by Tidy Tuesday, which includes an indicator for April 20th (4/20). We will investigate whether 4/20 sees more crashes than expected.\n\nImport & Process Data\n\nimport pandas as pd\n\n# load data\nraw_420 = pd.read_csv('data/daily_accidents_420.csv', parse_dates=['date'])\n\n# inspect data\nraw_420.head()\n\n\n\n\n\n\n\n\ndate\ne420\nfatalities_count\n\n\n\n\n0\n1992-01-01\nFalse\n144\n\n\n1\n1992-01-02\nFalse\n111\n\n\n2\n1992-01-07\nFalse\n85\n\n\n3\n1992-01-12\nFalse\n127\n\n\n4\n1992-01-03\nFalse\n182\n\n\n\n\n\n\n\n\n# count missing values\nraw_420.isna().sum()\n\ndate                 0\ne420                13\nfatalities_count     0\ndtype: int64\n\n\n\n# inspect missing values\nraw_420[raw_420['e420'].isna()]\n\n\n\n\n\n\n\n\ndate\ne420\nfatalities_count\n\n\n\n\n1099\n1994-04-20\nNaN\n1\n\n\n1428\n1995-04-20\nNaN\n2\n\n\n1834\n1996-04-20\nNaN\n1\n\n\n2201\n1997-04-20\nNaN\n2\n\n\n3301\n2000-04-20\nNaN\n1\n\n\n4400\n2003-04-20\nNaN\n1\n\n\n5485\n2006-04-20\nNaN\n1\n\n\n5867\n2007-04-20\nNaN\n1\n\n\n6234\n2008-04-20\nNaN\n1\n\n\n6968\n2010-04-20\nNaN\n2\n\n\n7702\n2012-04-20\nNaN\n1\n\n\n8393\n2014-04-20\nNaN\n1\n\n\n8802\n2015-04-20\nNaN\n1\n\n\n\n\n\n\n\n\ndf = (\n    raw_420\n    # convert e420 to boolean and rename\n    .assign(is_420=raw_420['e420'].astype(bool))\n    # drop missing values\n    .dropna()\n    # create boolean where TRUE if date is 07/04\n    .assign(is_4th_july=(raw_420['date'].dt.month == 7) & (raw_420['date'].dt.day == 4))\n    # select relevant columns\n    [['date', 'is_420', 'is_4th_july', 'fatalities_count']]\n)\n\n\ndf.head()\n\n\n\n\n\n\n\n\ndate\nis_420\nis_4th_july\nfatalities_count\n\n\n\n\n0\n1992-01-01\nFalse\nFalse\n144\n\n\n1\n1992-01-02\nFalse\nFalse\n111\n\n\n2\n1992-01-07\nFalse\nFalse\n85\n\n\n3\n1992-01-12\nFalse\nFalse\n127\n\n\n4\n1992-01-03\nFalse\nFalse\n182\n\n\n\n\n\n\n\n\n\nVisual Comparisons\n\ndf.groupby('is_420')['fatalities_count'].mean()\n\nis_420\nFalse    144.92477\nTrue      54.60000\nName: fatalities_count, dtype: float64\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(15, 8))\n\n# line plot of daily fatalities\nsns.lineplot(data=df, x='date', y='fatalities_count', color=\"#0081a7\", linewidth=0.5)\n\n# scatter plot for 4/20 days, filter using .loc to avoid NA issues\nsns.scatterplot(\n    data=df.loc[df['is_420'] == True],\n    x='date', y='fatalities_count',\n    color='#ef233c', label='4/20'\n    )\n\nplt.title('Daily Fatalities (1992-2016)')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(12, 6))\n\n# define colour palette\ncustom_palette = {False: '#0081a7', True: '#ef233c'}\n\n# histogram\nplt.subplot(1, 2, 1)\nsns.histplot(data=df, x='fatalities_count', hue='is_420', kde=True, palette=custom_palette)\nplt.title('Histogram')\n\n# boxplot\nplt.subplot(1, 2, 2)\nsns.boxplot(data=df, x='is_420', y='fatalities_count', hue='is_420', palette=custom_palette, legend=False)\nplt.xticks([0, 1], ['Other days', '4/20'])\nplt.title('Boxplot')\n\n# figure title\nplt.suptitle('Distribution of Fatalities', fontsize=14)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe imbalance between 4/20 and other days in the year makes it impossible to really see what is going on in our histogram. We can normalise the two distributions such that the total area of both equals one. This preserves their shape but accounts for the count imbalance between the two.\nSee the Seaborn documentation for more information: https://seaborn.pydata.org/generated/seaborn.histplot.html\nWe can also replace the boxplot with a violin plot, which will give us a little more intuition for the shape of the two groups.\n\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nsns.histplot(data=df, x='fatalities_count', hue='is_420', palette=custom_palette, kde=True, stat='density', common_norm=False)\nplt.title('Density-Normalised Histogram')\n\nplt.subplot(1, 2, 2)\nsns.violinplot(data=df, x='is_420', y='fatalities_count', inner='quart', hue='is_420', palette=custom_palette, legend=False)\nplt.xticks([0, 1], ['Other days', '4/20'])\nplt.title('Violin Plot')\n\nplt.suptitle('Distribution of Fatalities', fontsize=14)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nQuestion: What do you notice about the distribution (centre, spread, outliers, etc.)?\n\n\nTesting Comparisons\nVisual and descriptive comparisons are limited because they only tell us whether there is a difference. They don’t help us infer whether those difference occurred due to random variation or if there is something real going on. Visual comparisons cannot tell us whether we should expect to observe the differences we see in our samples in the population.\nThat’s where statistical tests come in! Once we’ve visualized potential differences, we can test whether they’re statistically significant, using a two-sample t-test.\nA t-test is a statistical test used to compare the means of two groups to determine if the difference between them is statistically significant. It takes into account:\n\nThe size of the difference between the two group means.\nThe variability (spread) of the data within each group.\nThe sample size (number of observations in each group).\n\nThe t-test calculates a p-value, which is the probability of observing a difference as extreme or more extreme than the one found, assuming there is no true difference between the groups in the population (i.e., the null hypothesis is true). If the p-value is small enough (below a threshold like 0.05), you reject the null hypothesis and conclude that the difference between the groups is likely real and not due to random chance.\n\n\nfrom scipy.stats import ttest_ind\n\n# create our samples for comparison\ngroup_420 = df.loc[df.is_420, 'fatalities_count']\ngroup_other = df.loc[~df.is_420, 'fatalities_count']\n\n# calculate t-statistic and p-value\nt_stat, p_val = ttest_ind(group_420, group_other, equal_var=False)\nprint(f\"t-statistic = {t_stat:.3f}, p-value = {p_val:.3f}\")\n\n# calculate mean difference\nmean_diff = group_420.mean() - group_other.mean()\n# calculate standard errors\nse_diff = np.sqrt(\n    group_420.var(ddof=1)/len(group_420)\n    + group_other.var(ddof=1)/len(group_other)\n    )\n\n# ci_lower = mean_diff - 1.96 * se_diff\n# ci_upper = mean_diff + 1.96 * se_diff\n# print(f\"Mean difference = {mean_diff:.2f} (95% CI: {ci_lower:.2f}, {ci_upper:.2f})\")\n\nprint(f\"Mean difference = {mean_diff:.2f}\")\n\nt-statistic = -29.369, p-value = 0.000\nMean difference = -90.32\n\n\nInterpretation:\n\nIf p &lt; 0.05, we reject the null that there is no true difference between the groups in the population.\nHere, the p-value is below 0.05 — suggesting that 4/20 days see significantly less fatal crashes than other days. The evidence supports a real difference.\n\n\n\nSimulation-Based Tests\nLet’s briefly replicate the comparison using a simulation-based method. This avoids strong distributional assumptions.\n\ndef simulate_two_groups(data1, data2):\n\n    n, m = len(data1), len(data2)\n    data = np.append(data1, data2)\n    np.random.shuffle(data)\n    group1 = data[:n]\n    group2 = data[n:]\n    return group1.mean() - group2.mean()\n\n\n# run 5000 simulations to test null\nnp.random.seed(42)\nsimulated_diffs = [simulate_two_groups(group_420, group_other) for _ in range(5000)]\n\n# observed mean difference\nobserved_diff = group_420.mean() - group_other.mean()\n\n# calculate p-value\ndiffs = np.array(simulated_diffs)\np_sim = np.mean(np.abs(diffs) &gt;= np.abs(observed_diff))\n\n# plot distribution of simulated differences with p-value\nplt.figure(figsize=(12, 6))\nsns.histplot(diffs, kde=True, color='#0081a7')\nplt.axvline(observed_diff, color='#ef233c', linewidth=3, linestyle=\"--\", label='Observed Mean Difference')\nplt.legend(loc='upper right')\nplt.title('Simulated Mean Differences (Permutation Test)')\n\n# annotate p-value on the plot\nplt.text(\n    x=observed_diff+5,\n    y=plt.gca().get_ylim()[1]*0.9,\n    s=f'p-value = {p_sim:.4f}'\n    )\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nTakeaway: Simulation confirms the result and emphasizes flexibility: even when assumptions are questionable, we can still test meaningfully. It also reinforces that inference is about what would happen if we repeated the experiment many times.",
    "crumbs": [
      "Sessions",
      "Regression - Comparing Samples"
    ]
  },
  {
    "objectID": "sessions/10-comparing-samples/index.html#limitations-of-comparison",
    "href": "sessions/10-comparing-samples/index.html#limitations-of-comparison",
    "title": "Comparison - Thief of Joy?",
    "section": "Limitations of Comparison",
    "text": "Limitations of Comparison\nComparing samples of data can be very useful. There is descriptive value in just knowing that differences exist in the data, and this may point to a meaningful difference in the population. However, if you are trying to understand what caused the differences between the two samples, comparison is not enough.\nQuestions: What might be wrong with our comparison?",
    "crumbs": [
      "Sessions",
      "Regression - Comparing Samples"
    ]
  },
  {
    "objectID": "sessions/10-comparing-samples/index.html#wrapping-up",
    "href": "sessions/10-comparing-samples/index.html#wrapping-up",
    "title": "Comparison - Thief of Joy?",
    "section": "Wrapping Up",
    "text": "Wrapping Up\n\nWorkflow recap: Start with a question → consider population vs. sample → visualize → compare → test → interpret.\nLimits of this approach: Group comparisons are powerful, but rigid. They don’t account for multiple variables or continuous predictors. Context and sample size also matter.\nLooking ahead: Next we will move from comparing means to analysing relationships between variables. That lets us ask new types of questions. We’ll explore how variables change together, detect trends, and lay the foundation for regression.\n\nPotential extension: Pick another potentially “special” date—e.g., July 4, New Year’s Day—and apply the same steps. Are there elevated crash counts there too? You can also explore other factors, like day of week or holiday status, to dig deeper into what might influence crash rates.",
    "crumbs": [
      "Sessions",
      "Regression - Comparing Samples"
    ]
  },
  {
    "objectID": "resources/ncf.html",
    "href": "resources/ncf.html",
    "title": "National Competency Framework",
    "section": "",
    "text": "We have mapped the contents of the syllabus to competencies in the National Competency Framework for Data Professionals in Health and Care so that you can see which sessions will help you on your way towards them.\nFor full details of the skills in the framework, the self-assessment tool can be found on FutureNHS.\nPlease note that this is a first draft of the mapping of NCF competencies to our syllabus and it is awaiting review.\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  Session Number\n  Session Date\n  Module\n  Session Name\n  Description\n  NCF Competency\n\n\n\n  \n    1\n    22/04/2025\n    N/A\n    Nectar Re-Launch\n    Showcasing the re-launch of Code Club\n    -\n  \n  \n    2\n    01/05/2025\n    On-boarding\n     Python On-boarding\n    What to install, basic virtual environment management, introduction to VS Code\n    SA21 : Python Proficiency\n  \n  \n    3\n    15/05/2025\n    On-boarding\n    Jupyter Notebooks\n    Demonstration of Jupyter Notebooks\n    SA21 : Python Proficiency\n  \n  \n    4\n    29/05/2025\n    Visualisation\n    EDA and Visualisations\n    Introduction to Exploratory Data Analysis and how to visualise it\n    SA4 : Descriptive and Explicative Analytics\n  \n  \n    5\n    12/06/2025\n    Visualisation\n    Visualisation with Seaborn\n    Aesthetically-pleasing visualisations\n    SA1 : Data Visualisation\n  \n  \n    6\n    26/06/2025\n    Core concepts\n    Data Types\n    Introduction to Data Types\n    SA21 : Python Proficiency\n  \n  \n    7\n    10/07/2025\n    Core concepts\n    Control Flow\n    Introduction to Control Flow\n    SA21 : Python Proficiency\n  \n  \n    8\n    24/07/2025\n    Core concepts\n    Functions & Functional Programming\n    Introduction to Functions and Functional Programming\n    SA21 : Python Proficiency\n  \n  \n    9\n    07/08/2025\n    Core concepts\n    Object-Oriented Programming\n    Introduction to Object-Oriented Programming\n    SA21 : Python Proficiency\n  \n  \n    10\n    21/08/2025\n    Visualisation\n    Streamlit dashboards\n    How to present data (visualisations) in a Streamlit dashboard\n    SA1 : Data Visualisation\n  \n  \n    11\n    04/09/2025\n    Data Science\n    Comparing Samples\n    Understanding data distributions and comparisons between samples\n    SA4 : Descriptive and Explicative Analytics\n  \n  \n    12\n    18/09/2025\n    Data Science\n    Analysing Relationships\n    Quantifying relationships with hypothesis tests and statistical significance\n    SA15 : Hypothesis Testing\n  \n  \n    13\n    02/10/2025\n    Data Science\n    Introduction to Linear Regression\n    Introduction to regression concepts and the component parts of linear regression\n    SA7 : Advanced Statistics\n  \n  \n    14\n    16/10/2025\n    Data Science\n    Implementing Linear Regression\n    Building linear models, assessing model fit, and interpreting coefficients\n    SA7 : Advanced Statistics\n  \n  \n    15\n    30/10/2025\n    Data Science\n    Beyond Linearity\n    Introduction to generalised linear regression models\n    SA7 : Advanced Statistics",
    "crumbs": [
      "Resources",
      "National Competency Framework"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "This page will have a list of definitions for commonly used (and/or commonly misunderstood) terminology and acronyms relating to python or data analysis and manipulation in general.\n\n\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n  Term\n  Definition\n  Notes and links\n\n\n\n  \n    Exploratory Data Analysis (EDA)\n    The process of analysing datasets to summarise their main characteristics, often using visual methods, before formal modeling.\n    \n  \n  \n    Functional programming\n    A programming paradigm that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data.\n    \n  \n  \n    Git\n    A distributed version control system used to track changes in source code during software development.\n    \n  \n  \n    Github\n    A cloud-based platform that provides hosting for repositories of files and folders (usually of software code) using git as its backend for version control.\n    website\n  \n  \n    JuPyter\n    A software package that allows the creation of python notebooks that can include a mixture of markdown-formatted text and live Python code.\n    \n  \n  \n    Markdown (.md)\n    A simple plain-text markup scheme designed to allow for rapid production of formatted text (with headings, links, etc) within a plaintext file. Used by Quarto (as a Quarto-specific flavour with the .qmd extension).\n    \n  \n  \n    matplotlib\n    The baseline Python library for creating static, animated, and interactive visualisations, offering extensive customisation options for plots and charts. Also used as a framework for more advanced or visually pleasing visualisation packages like seaborn.\n    \n  \n  \n    Object-oriented programming (OOP)\n    A programming paradigm based on the concept of \"objects,\" which can contain data and code to manipulate that data.\n    \n  \n  \n    pandas\n    A python data analysis and manipulation library for working with dataframes (tabular data)\n    website\n  \n  \n    Python\n    A general-purpose programming language\n    website\n  \n  \n    Regression\n    A method for modeling the relationship between a dependent variable and one or more independent variables. It it used to predictoutcomes and understand the impact of changes in predictor variables on the response variable.\n    \n  \n  \n    Repository (repo)\n    In git and github, a repository is a self-contained \"project\" of files and folders.\n    \n  \n  \n    Reproducible Analytical Pipelines (RAP)\n    A set of processes and tools designed to ensure that data analysis can be consistently repeated and verified by others.\n    \n  \n  \n    seaborn (sns)\n    A Python data visualisation library based on Matplotlib, providing a high-level interface for drawing attractive and informative statistical graphics.\n    website\n  \n  \n    TOML\n    Tom's Obvious Minimal Language - simple, human-readable data serialisation format designed for configuration files, emphasizing readability and ease of use. Used by uv to specify its projects.\n    \n  \n  \n    uv\n    A Python package manager, which can manage python projects (folders) and manage the installation and management of the python environment and libraries within that folder.\n    website\n  \n  \n    YAML\n    Yet Another Markup Language - a human-readable data serialisation format often used for configuration files and data exchange between languages.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#what-to-expect",
    "href": "sessions/01-onboarding/slides.html#what-to-expect",
    "title": "Python Onboarding",
    "section": "What to Expect?",
    "text": "What to Expect?\n\nLearning a language is hard. It can be frustrating. Perseverance is key to success.\nThese sessions will introduce you to Python, showing you what is possible and how to achieve some of what might benefit your work.\nBut the real learning comes by doing. You need to run the code yourself, have a play around, and cement what you’ve learned by applying it.\nPractice, repetition, and making mistakes along the way is how real progress is made."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#why-learn-python",
    "href": "sessions/01-onboarding/slides.html#why-learn-python",
    "title": "Python Onboarding",
    "section": "Why Learn Python?",
    "text": "Why Learn Python?\n\nCoding skills, generally, and Python specifically, seem to be a priority in the NHS right now. It’s a clear direction of travel. Learning now sets you up for the future.\nPython and the applied skills taught in these sessions will enable you to use advanced methods and design flexible, scalable solutions.\nPython is very valuable for career development.\nIt is (hopefully) fun!"
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#the-toolkit",
    "href": "sessions/01-onboarding/slides.html#the-toolkit",
    "title": "Python Onboarding",
    "section": "The Toolkit",
    "text": "The Toolkit\n\nWe will be using the following tools throughout this course:\n\nLanguage: Python\nDependency management: uv\nVersion Control: Git, GitHub Desktop\nIDE: VS Code/Jupyter Notebooks (or your preferred IDE)\n\nYou can install all these tools by running the following in PowerShell:\n\nwinget install astral-sh.uv Microsoft.VisualStudioCode github-desktop"
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#python",
    "href": "sessions/01-onboarding/slides.html#python",
    "title": "Python Onboarding",
    "section": "Python",
    "text": "Python\n\nPython is an all-purpose programming language that is the most popular worldwide and widely used in almost every industry.\nPython’s popularity is owed to its flexibility – it is the second-best tool for every job.\nIt is a strong choice for data science and analytics, being one of the best languages for data wrangling, data visualisation, statistics, and machine learning.\n\nIt is also well-suited to web development, scientific computing, and automation."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#dependency-management",
    "href": "sessions/01-onboarding/slides.html#dependency-management",
    "title": "Python Onboarding",
    "section": "Dependency Management",
    "text": "Dependency Management\n\nDependency management refers to the process of tracking and managing all of the packages (dependencies) a project needs to run. It ensures:\n\nThe right packages are installed.\nThe correct versions are used.\nConflicts between packages are avoided.\n\nWe are using uv for dependency management."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#virtual-environments",
    "href": "sessions/01-onboarding/slides.html#virtual-environments",
    "title": "Python Onboarding",
    "section": "Virtual Environments",
    "text": "Virtual Environments\n\nVirtual environments are isolated Python environments that allow you to manage dependencies for a specific project without the state of those dependencies affecting other projects or your wider system. They help by:\n\nKeeping dependencies separate for each project.\nAvoiding version conflicts between projects.\nMaking dependency management more predictable and reproducible.\n\nVirtual environments are a part of dependency management, and we will use uv to manage both the dependencies and virtual environments."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#version-control",
    "href": "sessions/01-onboarding/slides.html#version-control",
    "title": "Python Onboarding",
    "section": "Version Control",
    "text": "Version Control\n\nVersion control is the practice of tracking and managing changes to code or files over time, allowing you to:\n\nRevert to earlier versions if needed.\nCollaborate with others on the same project easily.\nMaintain a history of changes.\n\nWe are using Git (the version control system) and GitHub (the platform for hosting our work)."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#ide",
    "href": "sessions/01-onboarding/slides.html#ide",
    "title": "Python Onboarding",
    "section": "IDE",
    "text": "IDE\n\nAn IDE (Integrated Development Environment) is fully featured software that provides everything you need to write code as conveniently as possible.\nIt typically includes a code editor, debugger, build tools, and features like syntax highlighting and code completion.\nSome common IDEs used for Python include VS Code, PyCharm, Vim, Jupyter Notebooks/JupyterLab, and Positron.\nWe will use VS Code or Jupyter Notebooks (which is not exactly an IDE but is similar)."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "This is the schedule for Code Club in FY25/26.\nMaterials, including presentations and coding notebooks, have been made available in the associated GitHub repository and recordings of the sessions will be accessible via the Recording Link.\nThe Demonstration, Presentation, and Notebook columns indicate the content to be expected for each session:\n\nDemonstration: A live show-and-tell relating to the discussion topic.\nPresentation: A slide deck covering the discussion topic.\nNotebook: A Jupyter Notebook containing code-along elements or examples for people to work through after the session.\n\nTutorials will be divided into Modules. We recommend that people attend or watch tutorials in the Core module in order to gain a fundamental understanding of coding concepts and resources. Further modules are to be confirmed, but will likely include Automation, Dashboards and Visualisation, and Data Science. People will be able to attend those modules that interest them.\nSessions have been mapped to the National Competency Framework for Data Professionals in Health and Care, see more information here\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  Session Number\n  Session Date\n  Module\n  Session Name\n  Description\n  Demonstration\n  Presentation\n  Notebook\n  GitHub Link\n  Recording Link\n\n\n\n  \n    1\n    22/04/2025\n    N/A\n    Nectar Re-Launch\n    Showcasing the re-launch of Code Club\n    🎬\n    💻\n    -\n    Nectar Re-Launch\n    \n  \n  \n    2\n    01/05/2025\n    On-boarding\n     Python On-boarding\n    What to install, basic virtual environment management, introduction to VS Code\n    🎬\n    -\n    -\n    \n    \n  \n  \n    3\n    15/05/2025\n    On-boarding\n    Jupyter Notebooks\n    Demonstration of Jupyter Notebooks\n    🎬\n    -\n    📖\n    \n    \n  \n  \n    4\n    29/05/2025\n    Visualisation\n    EDA and Visualisations\n    Introduction to Exploratory Data Analysis and how to visualise it\n    🎬\n    -\n    📖\n    \n    \n  \n  \n    5\n    12/06/2025\n    Visualisation\n    Visualisation with Seaborn\n    Aesthetically-pleasing visualisations\n    🎬\n    -\n    📖\n    \n    \n  \n  \n    6\n    26/06/2025\n    Core concepts\n    Data Types\n    Introduction to Data Types\n    -\n    💻\n    📖\n    \n    \n  \n  \n    7\n    10/07/2025\n    Core concepts\n    Control Flow\n    Introduction to Control Flow\n    -\n    💻\n    📖\n    \n    \n  \n  \n    8\n    24/07/2025\n    Core concepts\n    Functions & Functional Programming\n    Introduction to Functions and Functional Programming\n    -\n    💻\n    📖\n    \n    \n  \n  \n    9\n    07/08/2025\n    Core concepts\n    Object-Oriented Programming\n    Introduction to Object-Oriented Programming\n    -\n    💻\n    📖\n    \n    \n  \n  \n    10\n    21/08/2025\n    Visualisation\n    Streamlit dashboards\n    How to present data (visualisations) in a Streamlit dashboard\n    🎬\n    -\n    -\n    \n    \n  \n  \n    11\n    04/09/2025\n    Data Science\n    Comparing Samples\n    Understanding data distributions and comparisons between samples\n    -\n    💻\n    📖\n    \n    \n  \n  \n    12\n    18/09/2025\n    Data Science\n    Analysing Relationships\n    Quantifying relationships with hypothesis tests and statistical significance\n    -\n    💻\n    📖\n    \n    \n  \n  \n    13\n    02/10/2025\n    Data Science\n    Introduction to Linear Regression\n    Introduction to regression concepts and the component parts of linear regression\n    -\n    💻\n    📖\n    \n    \n  \n  \n    14\n    16/10/2025\n    Data Science\n    Implementing Linear Regression\n    Building linear models, assessing model fit, and interpreting coefficients\n    🎬\n    💻\n    📖\n    \n    \n  \n  \n    15\n    30/10/2025\n    Data Science\n    Beyond Linearity\n    Introduction to generalised linear regression models\n    🎬\n    💻\n    📖",
    "crumbs": [
      "Schedule"
    ]
  }
]