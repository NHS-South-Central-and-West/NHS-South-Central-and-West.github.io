[
  {
    "objectID": "resources/glossary.html",
    "href": "resources/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "This page will have a list of definitions for commonly used (and/or commonly misunderstood) terminology and acronyms relating to python or data analysis and manipulation in general.\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n  Term\n  Definition\n\n\n\n  \n    Exploratory Data Analysis (EDA)\n    The process of analysing datasets to summarise their main characteristics, often using visual methods, before formal modeling.\n  \n  \n    Functional programming\n    A programming paradigm that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data.\n  \n  \n    Git\n    A distributed version control system used to track changes in source code during software development.\n  \n  \n    GitHub\n    A cloud-based platform that provides hosting for repositories of files and folders (usually of software code) using git as its backend for version control.\n  \n  \n    Jupyter\n    A software package that allows the creation of python notebooks that can include a mixture of markdown-formatted text and live Python code.\n  \n  \n    Markdown (.md)\n    A simple plain-text markup scheme designed to allow for rapid production of formatted text (with headings, links, etc) within a plaintext file. Used by Quarto (as a Quarto-specific flavour with the .qmd extension).\n  \n  \n    matplotlib\n    The baseline Python library for creating static, animated, and interactive visualisations, offering extensive customisation options for plots and charts. Also used as a framework for more advanced or visually pleasing visualisation packages like seaborn.\n  \n  \n    Object-oriented programming (OOP)\n    A programming paradigm based on the concept of \"objects,\" which can contain data and code to manipulate that data.\n  \n  \n    pandas\n    A python data analysis and manipulation library for working with dataframes (tabular data)\n  \n  \n    Python\n    A general-purpose programming language\n  \n  \n    Regression\n    A method for modeling the relationship between one or more explanatory variables and an outcome. It is used to predict outcomes and understand the impact of changes in predictors (explanatory variables) on the response (outcome).\n  \n  \n    Repository (repo)\n    In git and github, a repository is a self-contained \"project\" of files and folders.\n  \n  \n    Reproducible Analytical Pipelines (RAP)\n    A set of processes and tools designed to ensure that data analysis can be consistently repeated and verified by others.\n  \n  \n    seaborn (sns)\n    A Python data visualisation library based on Matplotlib, providing a high-level interface for drawing attractive and informative statistical graphics.\n  \n  \n    TOML\n    Tom's Obvious Minimal Language - simple, human-readable data serialisation format designed for configuration files, emphasizing readability and ease of use. Used by uv to specify its projects.\n  \n  \n    uv\n    A Python package manager, which can manage python projects (folders) and manage the installation and management of the python environment and libraries within that folder.\n  \n  \n    YAML\n    Yet Another Markup Language - a human-readable data serialisation format often used for configuration files and data exchange between languages."
  },
  {
    "objectID": "sessions/01-onboarding/index.html",
    "href": "sessions/01-onboarding/index.html",
    "title": "Python Onboarding",
    "section": "",
    "text": "This is the first session of Code Club’s relaunch. It focuses on giving users all the tools they need to get started using Python and demonstrates the setup for a typical Python project.",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "1. Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html#session-slides",
    "href": "sessions/01-onboarding/index.html#session-slides",
    "title": "Python Onboarding",
    "section": "Session Slides",
    "text": "Session Slides\nUse the left ⬅️ and right ➡️ arrow keys to navigate through the slides below. To view in a separate tab/window, follow this link.",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "1. Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html#the-tools-you-will-need",
    "href": "sessions/01-onboarding/index.html#the-tools-you-will-need",
    "title": "Python Onboarding",
    "section": "The Tools You Will Need",
    "text": "The Tools You Will Need\nWhile this course focuses on Python, we will use several other tools throughout.\n\nLanguage: Python\nDependency Management & Virtual Environments: uv\nVersion Control: Git, GitHub Desktop\nIDE: VS Code/Jupyter Notebooks (or your preferred IDE)\n\nYou can install all the tools you’ll need by running the following one-liner run in PowerShell:\nwinget install astral-sh.uv Microsoft.VisualStudioCode github-desktop\n\nPython\nPython is an all-purpose programming language that is one of, if not the most popular, in the world1 and is widely used in almost every industry. Its popularity is owed to its flexibility as a language that can be used to achieve nearly any job. It is often referred to as the second-best tool for every job. Specialist languages might be better for specific tasks (for example, R for statistics), but Python is good at everything.\nIt is a strong choice for data science and analytics, being one of the best languages for data wrangling, data visualisation, statistics, and machine learning. It is also well-suited to web development, scientific computing, and automation.\n\n\nDependency Management\nOne of Python’s greatest weaknesses is dependency management. Despite its many strengths, there is no escaping the dependency hell that every Python user faces.\nDependency management refers to the process of tracking and managing all of the packages (dependencies) a project needs to run. It is a consideration in any programming language. It ensures:\n\nThe right packages are installed.\nThe correct versions are used.\nConflicts between packages are avoided.\n\nThere are many reasons that Python handles dependency management so poorly, but there are some tools that make this a little easier on users. We are using uv for dependency management. It is relatively new, but it is quickly becoming the consensus tool for dependency management in Python because it makes the process about as painless as it can be without moving to a different language entirely.\n\nVirtual Environments\nVirtual environments are a component of dependency management. Dependency management becomes much messier when you have many Python projects, each using their own packages, some overlapping and some requiring specific versions, either for compatibility or functionality reasons. Reducing some of this friction by isolating each project in its own virtual environment, like each project is walled off from all other projects, makes dependency management a little easier. Virtual environments allow you to manage dependencies for a specific project without the state of those dependencies affecting other projects or your wider system.\nVirtual environments help by:\n\nKeeping dependencies separate for each project.\nAvoiding version conflicts between projects.\nMaking dependency management more predictable and reproducible.\n\nWe will use uv to manage all dependencies, virtual environments, and even versions of Python.\n\n\n\nVersion Control\nVersion control is the practice of tracking and managing changes to code or files over time, allowing you to:\n\nRevert to earlier versions if needed.\nCollaborate with others on the same project easily.\nMaintain a history of changes.\n\nWe are using Git, a version control system, to host our work and GitHub Desktop to manage version control locally.\nVersion control and Git are topics entirely in their own right, and covering them in detail is out of the scope of this session. We hope to cover version control in a future session, but right now, you just need to be able to access materials for these sessions. You can find the materials in the Code Club repository.\nIf you have downloaded GitHub Desktop, the easiest way to access these materials and keep up-to-date is by cloning the Code Club repository (go to File, then Clone Repository, select URL, and paste the Code Club repository link in the URL field). You can then ensure that the materials you are using are the most current by clicking the Fetch Origin button in GitHub Desktop, which grabs the changes we’ve made from the central repository on GitHub.\n\n\nIDE\nIDEs (Integrated Development Environments) are software that simplifies programming and development by combining many of the most common tasks and helpful features for programming into a single tool. These typically include a code editor, debugging functionality, build tools, and features like syntax highlighting and code completion. When you start your code journey, you might not need all these tools, and fully-featured IDEs can be overwhelming. But as you become more comfortable with programming, all these different features will become very valuable.\nSome common IDEs that are used for Python include:\n\nVS Code\nPyCharm\nVim\nJupyter Notebooks/JupyterLab\nPositron\n\nWe will use VS Code or Jupyter Notebooks (which is not exactly an IDE but is similar).",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "1. Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html#project-setup",
    "href": "sessions/01-onboarding/index.html#project-setup",
    "title": "Python Onboarding",
    "section": "Project Setup",
    "text": "Project Setup\nEvery new Python project should start with using uv to set up a virtual environment for the project to keep everything organised and reduce the risk of finding yourself in dependency hell.\nThe entire project setup process can be handled in the command line. We will use PowerShell for consistency.\nWhen you open a PowerShell window, it should open in your C drive (e.g.,  C:\\Users\\user.name). If it does not, run cd ~, and it should return to your home directory.\nWe will create a new uv project in the home directory2 using the command uv init. The new project will contain everything we need, including a Python installation, a virtual environment, and the necessary project files for tracking and managing any packages installed in the virtual environment. To set up a new project called test-project, use the following command:\nuv init test-project\nHaving created this new directory, navigate to it using cd test-project. You can check the files in a directory using the command ls. If you run this command, you will see three files in the project directory (hello.py, pyproject.toml, and README.md). The project doesn’t yet have a Python installation or a virtual environment, but this will be added when we add external Python packages.\nYou can install Python packages using the command uv add. We can add some common Python packages that we will use in most projects (pandas, numpy, seaborn, and ipykernel3) using the following command:\nuv add pandas numpy seaborn ipykernel\nThe output from this command will reference the Python installation used and the creation of a virtual environment directory .venv. Now, if you run ls, you will see two new items in the directory, uv.lock and .venv.\nYour Python project is now set up, and you are ready to start writing some code. You can open VS Code from your PowerShell window by running code ..\nFor more information about creating and managing projects using uv, check out the uv documentation.\n\nOpening your project in VS Code\nTo open your newly-created uv project in VS Code, launch the application and click File &gt; Open Folder.... You’ll want to make sure you select the root level of your project. Once you’ve opened the folder, the file navigation pane in VS Code should display the files that uv has created, including a main.py example file. Click on this to open it.\nOnce VS Code realises you’ve opened a folder with Python code and a virtual environment, it should do the following:\n\nSuggest you install the Python extension (and, once you’ve created a Jupyter notebook, the Jupyter one) offered by Microsoft - go ahead and do this. If this doesn’t happen, you can install extensions manually from the Extensions pane on the left-hand side.\nSelect the uv-created .venv as the python Environment we’re going to use to actually run our code. If this doesn’t happen, press ctrl-shift-P, type “python environment” to find the Python - Create Environment... option, hit enter, choose “Venv” and proceed to “Use Existing”.\n\nIf all has gone well, you should be able to hit the “play” icon in the top right to execute main.py. The Terminal pane should open up below and display something like Hello from (your-project-name)!.",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "1. Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html#footnotes",
    "href": "sessions/01-onboarding/index.html#footnotes",
    "title": "Python Onboarding",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlthough there are several ways to measure language popularity, the PYPL Index, HackerRank’s Developer Skills Report, and IEEE Spectrum all rank Python as the most popular language in the world, while Stack Overflow’s Developer Survey places Python third behind JavaScript and HTML/CSS.↩︎\nWe recommend using the C drive for all Python projects, especially if using version control. Storing projects like these on One Drive will create many unnecessary issues.↩︎\nStrictly speaking, we should install ipykernel as a development dependency (a dependency that is needed for any development but not when the project is put into production). In this case, we would add it by running uv add --dev ipykernel. However, in this case, it is simpler to just add it as a regular dependency, and it doesn’t harm.↩︎",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "1. Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/index.html",
    "href": "sessions/02-jupyter_notebooks/index.html",
    "title": "Jupyter Notebooks",
    "section": "",
    "text": "This is the second session following Code Club’s relaunch. The focus is introducing jupyter notebooks and explaining to users how to get started with a new project and briefly introducing some key concepts.\nWe are also planning some time for Q&A following the first session.",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "2. Jupyter Notebooks"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/index.html#session-slides",
    "href": "sessions/02-jupyter_notebooks/index.html#session-slides",
    "title": "Jupyter Notebooks",
    "section": "Session Slides",
    "text": "Session Slides\nUse the left ⬅️ and right ➡️ arrow keys to navigate through the slides below. To view in a separate tab/window, follow this link.",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "2. Jupyter Notebooks"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/index.html#the-tools-you-will-need",
    "href": "sessions/02-jupyter_notebooks/index.html#the-tools-you-will-need",
    "title": "Jupyter Notebooks",
    "section": "The Tools You Will Need",
    "text": "The Tools You Will Need\nThough Jupyter notebooks can be used with a variety of coding languages and in different settings the key tools used in this session are:\n\nLanguage: Python\nDependency Management & Virtual Environments: uv\nVersion Control: Git, GitHub Desktop\nIDE: VS Code/Jupyter Notebooks (or your preferred IDE)\n\nYou can install all the tools you’ll need by running the following one-liner run in PowerShell:\nwinget install astral-sh.uv Microsoft.VisualStudioCode github-desktop\nYou can find more information on these topics in the Python Onboarding session",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "2. Jupyter Notebooks"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/index.html#project-setup",
    "href": "sessions/02-jupyter_notebooks/index.html#project-setup",
    "title": "Jupyter Notebooks",
    "section": "Project Setup",
    "text": "Project Setup\nOur project set-up will follow the same steps as used in the onboarding session, by using uv to set up a new project folder.\nTo get started we will use PowerShell powershell to open a command prompt, it should open in your C drive (e.g., C:\\Users\\user.name). If it does not, run cd ~, and it should return to your home directory. We recommend the use of a single folder to hold your python projects while learning, because we will be using git version control we will call this “Git”. we can use the command mkdir code_club to make this folder and then use cd code_club to relocate to this folder1.\nWe will create a new uv project in this directory using the command uv init. The new project will contain everything we need, including a Python installation, a virtual environment, and the necessary project files for tracking and managing any packages installed in the virtual environment. To set up a new project called test-project, use the following command:\nuv init test_project\nHaving created this new directory, navigate to it using cd test_project.\nFor this session you will need to add 3 Python packages, ipykernel2, pandas and seaborn We can use the following command:\nuv add ipykernel pandas seaborn\nWe are going to create a blank notebook in this file by running the command new-item first_notebook.ipynb if you now run ls you will note this file has been created\nYour Python project is now set up, and you are ready to start writing some code. You can open VS Code from your PowerShell window by running code ..\n\nOpening your project in VS Code\nYou could also do this from within VS Code as most IDEs include a terminal interface which will be demonstrated in session.\nFor now launch VS Code and click File &gt; Open Folder.... You’ll want to make sure you select the root level of your project. Once you’ve opened the folder, the file navigation pane in VS Code should display the files that uv has created, as well as the notebook you created: first_notebook.ipynb. Click on this to open it.\nOnce VS Code realises you’ve opened a folder with Python code and a virtual environment, it should do the following:\n\nSuggest you install the Python extension (and, once you’ve created a Jupyter notebook, the Jupyter one) offered by Microsoft - go ahead and do this. If this doesn’t happen, you can install extensions manually from the Extensions pane on the left-hand side.\nSelect the uv-created .venv as the python Environment we’re going to use to actually run our code. If this doesn’t happen, press ctrl-shift-P, type “python environment” to find the Python - Create Environment... option, hit enter, choose “Venv” and proceed to “Use Existing”.\n\nIf VS Code has found the virtual environment, it may pick up the correct kernel. If not you may need to select this manually this can be done by clicking in the top right where you can see Select Kernel (see below)\n\n\n\nClick ‘Select Kernel’\n\n\nWe can then select the appropriate kernel from python environments and looking for\n\n\n\nclick Python Environments\n\n\n\n\n\nclick venv - recommended\n\n\nOnce the kernel is enabled you are ready to start adding cells to your notebook. these can either be code cells which is where you include your program elements or markdown which enable the addition of headings, analysis and commentary.",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "2. Jupyter Notebooks"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/index.html#footnotes",
    "href": "sessions/02-jupyter_notebooks/index.html#footnotes",
    "title": "Jupyter Notebooks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe recommend using the C drive for all Python projects, especially if using version control. Storing projects like these on One Drive will create many unnecessary issues. It can be helpful to use a sub-directory to store projects but is not necessary and is not a requirement for code club↩︎\nStrictly speaking, we should install ipykernel as a development dependency (a dependency that is needed for any development but not when the project is put into production). In this case, we would add it by running uv add --dev ipykernel. However, in this case, it is simpler to just add it as a regular dependency, and it doesn’t harm.↩︎",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "2. Jupyter Notebooks"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/solutions.html",
    "href": "sessions/03-eda-pandas/solutions.html",
    "title": "EDA Exercise Solutions",
    "section": "",
    "text": "# import packages\nimport numpy as np\nimport pandas as pd\n\n\n# import the dataset\ndf = pd.read_csv('data/weatherAUS.csv')\n\n\nWhat is the ‘Sunshine’ column’s data type?\n\n\n# What is the 'Sunshine' column's data type?\ndf['Sunshine'].dtypes\n\ndtype('float64')\n\n\n\nIdentify all the columns that are of dtype ‘object’.\n\n\n# Identify all the columns that are of dtype 'object'\nlist(df.select_dtypes(include=['object']))\n\n['Date',\n 'Location',\n 'WindGustDir',\n 'WindDir9am',\n 'WindDir3pm',\n 'RainToday',\n 'RainTomorrow']\n\n\n\nHow many of the dataframe’s columns are of dtype ‘object’?\n\n\n# How many of the dataframe's columns are of dtype 'object'?\nlen(list(df.select_dtypes(include=['object'])))\n\n7\n\n\n\nHow many of the ‘Rainfall’ column values are NAs?\n\n\n# How many of the 'Rainfall' column values are NAs?\ndf['Rainfall'].isna().sum()\n\nnp.int64(3261)\n\n\n\nCreate a new dataframe which only includes the ‘Date’, ‘Location, ’Sunshine’, ‘Rainfall’, and ‘RainTomorrow’ columns.\n\n\n# Create a new dataframe which only includes the 'Date', 'Location, 'Sunshine', 'Rainfall', and 'RainTomorrow' columns.\nnew_df = df[['Date', 'Location', 'Sunshine', 'Rainfall', 'RainTomorrow']]\nnew_df.head()\n\n\n\n\n\n\n\n\nDate\nLocation\nSunshine\nRainfall\nRainTomorrow\n\n\n\n\n0\n2008-12-01\nAlbury\nNaN\n0.6\nNo\n\n\n1\n2008-12-02\nAlbury\nNaN\n0.0\nNo\n\n\n2\n2008-12-03\nAlbury\nNaN\n0.0\nNo\n\n\n3\n2008-12-04\nAlbury\nNaN\n0.0\nNo\n\n\n4\n2008-12-05\nAlbury\nNaN\n1.0\nNo\n\n\n\n\n\n\n\n\nConvert ‘RainTomorrow’ to a numeric variable, where ‘Yes’ = 1 and ‘No’ = 0.\n\n\n# Convert 'RainTomorrow' to a numeric variable, where 'Yes' = 1 and 'No' = 0.\n# df['Location'].astype('category').cat.codes\n# df['RainTomorrow'].astype('category').cat.codes\ndf['RainTomorrow'].map({'Yes': 1, 'No': 0})\n\n0         0.0\n1         0.0\n2         0.0\n3         0.0\n4         0.0\n         ... \n145455    0.0\n145456    0.0\n145457    0.0\n145458    0.0\n145459    NaN\nName: RainTomorrow, Length: 145460, dtype: float64\n\n\n\nWhat is the average amount of rainfall for each location?\n\n\n# What is the average amount of rainfall for each location?\ndf.groupby('Location')['Rainfall'].mean().sort_values(ascending=False)\n\nLocation\nCairns              5.742035\nDarwin              5.092452\nCoffsHarbour        5.061497\nGoldCoast           3.769396\nWollongong          3.594903\nWilliamtown         3.591108\nTownsville          3.485592\nNorahHead           3.387299\nSydney              3.324543\nMountGinini         3.292260\nKatherine           3.201090\nNewcastle           3.183892\nBrisbane            3.144891\nNorfolkIsland       3.127665\nSydneyAirport       3.009917\nWalpole             2.906846\nWitchcliffe         2.895664\nPortland            2.530374\nAlbany              2.263859\nBadgerysCreek       2.193101\nPenrith             2.175304\nTuggeranong         2.164043\nDartmoor            2.146567\nRichmond            2.138462\nMountGambier        2.087562\nLaunceston          2.011988\nAlbury              1.914115\nPerth               1.906295\nMelbourne           1.870062\nWatsonia            1.860820\nPerthAirport        1.761648\nCanberra            1.741720\nBallarat            1.740026\nWaggaWagga          1.709946\nPearceRAAF          1.669080\nMoree               1.630203\nBendigo             1.619380\nHobart              1.601819\nAdelaide            1.566354\nSale                1.510167\nMelbourneAirport    1.451977\nNuriootpa           1.390343\nCobar               1.127309\nSalmonGums          1.034382\nMildura             0.945062\nNhil                0.934863\nAliceSprings        0.882850\nUluru               0.784363\nWoomera             0.490405\nName: Rainfall, dtype: float64\n\n\n\nWhat is the average amount of rainfall for days that it will rain tomorrow?\n\n\n# What is the average amount of rainfall for days that it will rain tomorrow?\ndf.groupby('RainTomorrow')['Rainfall'].mean()\n\nRainTomorrow\nNo     1.270290\nYes    6.142104\nName: Rainfall, dtype: float64\n\n\n\nWhat is the average amount of sunshine in Perth when it will not rain tomorrow?\n\n\n# What is the average amount of sunshine in Perth when it will not rain tomorrow?\ndf.loc[(df['Location'] == 'Perth') & (df['RainTomorrow'] == 'No'), 'Sunshine'].mean()\n# df[(df['Location']=='Perth') & (df['RainTomorrow']=='No')]['Sunshine'].mean()\n\nnp.float64(9.705306603773584)\n\n\n\nWe want to understand the role that time plays in the dataset. Using the original dataframe, carry the following tasks and answer the corresponding questions:\n\nCreate columns representing the year and month from the ‘Date’ column. How many years of data are in the dataset?\nExamine the distribution of the ‘Sunshine’ NAs over time. Is time a component in the ‘Sunshine’ data quality issues?\nCalculate the average rainfall and sunshine by month. How do rainfall and sunshine vary through the year?\nCalculate the average rainfall and sunshine by year. How have rainfall and sunshine changed over time?\n\n\n\n# Create columns representing the year and month from the 'Date' column. How many years of data are in the dataset?\ndf = (\n    df.assign(Date=pd.to_datetime(df['Date']))\n    .assign(\n        Year=lambda x: x['Date'].dt.year,\n        Month=lambda x: x['Date'].dt.month\n    )\n)\n\ndf['Year'].nunique()\n\n11\n\n\n\n# Examine the distribution of the 'Sunshine' NAs over time. Is time a component in the 'Sunshine' data quality issues?\ndf.groupby('Year')['Sunshine'].apply(lambda x: x.isna().sum())\n\nYear\n2007        0\n2008      323\n2009     6146\n2010     6220\n2011     6053\n2012     6539\n2013     7570\n2014     9157\n2015     9441\n2016    11994\n2017     6392\nName: Sunshine, dtype: int64\n\n\n\n# Calculate the average rainfall and sunshine by month. How do rainfall and sunshine vary through the year?\ndf.groupby('Month')[['Rainfall', 'Sunshine']].mean()\n\n\n\n\n\n\n\n\nRainfall\nSunshine\n\n\nMonth\n\n\n\n\n\n\n1\n2.738478\n9.203301\n\n\n2\n3.188665\n8.590477\n\n\n3\n2.814450\n7.627229\n\n\n4\n2.334823\n7.080942\n\n\n5\n2.001654\n6.321179\n\n\n6\n2.782182\n5.645576\n\n\n7\n2.184209\n6.052353\n\n\n8\n2.034983\n7.136923\n\n\n9\n1.888543\n7.685301\n\n\n10\n1.614732\n8.490807\n\n\n11\n2.268177\n8.666098\n\n\n12\n2.491835\n8.997794\n\n\n\n\n\n\n\n\n# Calculate the average rainfall and sunshine by year. How have rainfall and sunshine changed over time?\ndf.groupby('Year')[['Rainfall', 'Sunshine']].mean()\n\n\n\n\n\n\n\n\nRainfall\nSunshine\n\n\nYear\n\n\n\n\n\n\n2007\n3.219672\n8.086885\n\n\n2008\n2.293541\n7.789111\n\n\n2009\n2.166385\n7.905102\n\n\n2010\n2.710924\n7.277599\n\n\n2011\n2.829197\n7.313705\n\n\n2012\n2.416200\n7.580733\n\n\n2013\n2.272402\n7.663279\n\n\n2014\n1.966341\n7.793905\n\n\n2015\n2.160753\n7.689140\n\n\n2016\n2.384054\n7.646902\n\n\n2017\n2.478834\n7.676602"
  },
  {
    "objectID": "sessions/schedule.html",
    "href": "sessions/schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "This is the schedule for Code Club in FY25/26.\nThe Demonstration, Presentation, and Notebook columns indicate the content to be expected for each session:\n\nDemonstration: A live show-and-tell relating to the discussion topic.\nPresentation: A slide deck covering the discussion topic.\nNotebook: A Jupyter Notebook containing code-along elements or examples for people to work through after the session.\n\nTutorials will be divided into Modules. We recommend that people attend or watch tutorials in the Core module in order to gain a fundamental understanding of coding concepts and resources. Further modules are to be confirmed, but will likely include Automation, Dashboards and Visualisation, and Data Science. People will be able to attend those modules that interest them.\nWe have mapped the contents of the syllabus to competencies in the National Competency Framework for Data Professionals in Health and Care so that you can see which sessions will help you on your way towards them. For full details of the skills in the framework, the self-assessment tool can be found on FutureNHS.\nPlease note that this is a first draft of the mapping of NCF competencies to our syllabus and it is awaiting review.\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  Session Date\n  Module\n  Session Name\n  Description\n  Demonstration\n  Presentation\n  Notebook\n  NCF Competency\n\n\n\n  \n    22/04/2025\n    N/A\n    Nectar Re-Launch\n    Showcasing the re-launch of Code Club\n    🎬\n    💻\n    -\n    -\n  \n  \n    01/05/2025\n    On-boarding\n     Python On-boarding\n    What to install, basic virtual environment management, introduction to VS Code\n    🎬\n    -\n    -\n    SA21 : Python Proficiency\n  \n  \n    15/05/2025\n    On-boarding\n    Jupyter Notebooks\n    Demonstration of Jupyter Notebooks\n    🎬\n    -\n    📖\n    SA21 : Python Proficiency\n  \n  \n    29/05/2025\n    Visualisation\n    EDA and Visualisations\n    Introduction to Exploratory Data Analysis and how to visualise it\n    🎬\n    -\n    📖\n    SA4 : Descriptive and Explicative Analytics\n  \n  \n    12/06/2025\n    Visualisation\n    Visualisation with Seaborn\n    Aesthetically-pleasing visualisations\n    🎬\n    -\n    📖\n    SA1 : Data Visualisation\n  \n  \n    26/06/2025\n    Core concepts\n    Data Types\n    Introduction to Data Types\n    -\n    💻\n    📖\n    SA21 : Python Proficiency\n  \n  \n    10/07/2025\n    Core concepts\n    Control Flow\n    Introduction to Control Flow\n    -\n    💻\n    📖\n    SA21 : Python Proficiency\n  \n  \n    24/07/2025\n    Core concepts\n    Functions & Functional Programming\n    Introduction to Functions and Functional Programming\n    -\n    💻\n    📖\n    SA21 : Python Proficiency\n  \n  \n    07/08/2025\n    Core concepts\n    Object-Oriented Programming\n    Introduction to Object-Oriented Programming\n    -\n    💻\n    📖\n    SA21 : Python Proficiency\n  \n  \n    21/08/2025\n    Visualisation\n    Streamlit dashboards\n    How to present data (visualisations) in a Streamlit dashboard\n    🎬\n    -\n    -\n    SA1 : Data Visualisation\n  \n  \n    04/09/2025\n    Data Science\n    Comparing Samples\n    Understanding data distributions and comparisons between samples\n    -\n    💻\n    📖\n    SA4 : Descriptive and Explicative Analytics\n  \n  \n    18/09/2025\n    Data Science\n    Analysing Relationships\n    Quantifying relationships with hypothesis tests and statistical significance\n    -\n    💻\n    📖\n    SA15 : Hypothesis Testing\n  \n  \n    02/10/2025\n    Data Science\n    Introduction to Linear Regression\n    Introduction to regression concepts and the component parts of linear regression\n    -\n    💻\n    📖\n    SA7 : Advanced Statistics\n  \n  \n    16/10/2025\n    Data Science\n    Implementing Linear Regression\n    Building linear models, assessing model fit, and interpreting coefficients\n    🎬\n    💻\n    📖\n    SA7 : Advanced Statistics\n  \n  \n    30/10/2025\n    Data Science\n    Beyond Linearity\n    Introduction to generalised linear regression models\n    🎬\n    💻\n    📖\n    SA7 : Advanced Statistics",
    "crumbs": [
      "Sessions",
      "Session Schedule"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html",
    "href": "sessions/03-eda-pandas/index.html",
    "title": "Exploring Data Using Pandas",
    "section": "",
    "text": "This is the first of four sessions looking at how to explore data in Python. This session will focus on introducing the Python library, pandas. We will use pandas\nWe are using Australian weather data, taken from Kaggle. To download the data, click here.\n# install necessary packages\n!uv add skimpy\n\n# import packages\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom skimpy import skim\n# import the dataset\ndf = pd.read_csv('data/weatherAUS.csv')",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#setting-the-scene",
    "href": "sessions/03-eda-pandas/index.html#setting-the-scene",
    "title": "Exploring Data Using Pandas",
    "section": "Setting the Scene",
    "text": "Setting the Scene\nBefore we start to explore any dataset, we need to establish what we are looking to do with the data. This should inform our decisions wwith any exploration, and any analysis that follows.\nFirst, we need to ask: - What are we trying to achieve? - How do our goals impact our analysis? - What should we take into consideration before we write any code? - What sort of questions might we be interested in with this dataset?\n\nWhat Our Data Can Tell Us (And What it Can’t)\nWe also need to consider what the data is and where it came from.\n\nHow was the data collected?\nWhat is it missing?\nWhat do the variables in our dataset actually mean, and are they a good approximation of the concepts we are interested in?\n\n\n\n\nPopulation vs Sample\n\nWhat is the difference between population data and sample data?\nWhy do we care?\nHow do methods/approaches differ when dealing with population data versus sample data?\n\n\n\nDescription vs Explanation (Inference) vs Prediction\n\nWhat do these different types of analysis mean?\nDo they impact the way we structure our analysis?\nDo they impact the code we write?",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#describing-data",
    "href": "sessions/03-eda-pandas/index.html#describing-data",
    "title": "Exploring Data Using Pandas",
    "section": "Describing Data",
    "text": "Describing Data\n\nWhat do we want to know about a dataset when we first encounter it?\nHow do we get a quick overview of the data that can help us in our next steps?\nWe need to get a “feel” for the data before we can really make any decisions about how to analyse it. How do we get there with a new dataset?\n\n\n# view the top five rows\ndf.head()\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n0\n2008-12-01\nAlbury\n13.4\n22.9\n0.6\nNaN\nNaN\nW\n44.0\nW\n...\n71.0\n22.0\n1007.7\n1007.1\n8.0\nNaN\n16.9\n21.8\nNo\nNo\n\n\n1\n2008-12-02\nAlbury\n7.4\n25.1\n0.0\nNaN\nNaN\nWNW\n44.0\nNNW\n...\n44.0\n25.0\n1010.6\n1007.8\nNaN\nNaN\n17.2\n24.3\nNo\nNo\n\n\n2\n2008-12-03\nAlbury\n12.9\n25.7\n0.0\nNaN\nNaN\nWSW\n46.0\nW\n...\n38.0\n30.0\n1007.6\n1008.7\nNaN\n2.0\n21.0\n23.2\nNo\nNo\n\n\n3\n2008-12-04\nAlbury\n9.2\n28.0\n0.0\nNaN\nNaN\nNE\n24.0\nSE\n...\n45.0\n16.0\n1017.6\n1012.8\nNaN\nNaN\n18.1\n26.5\nNo\nNo\n\n\n4\n2008-12-05\nAlbury\n17.5\n32.3\n1.0\nNaN\nNaN\nW\n41.0\nENE\n...\n82.0\n33.0\n1010.8\n1006.0\n7.0\n8.0\n17.8\n29.7\nNo\nNo\n\n\n\n\n5 rows × 23 columns\n\n\n\n\n# view the bottom ten rows\ndf.tail(10)\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n145450\n2017-06-16\nUluru\n5.2\n24.3\n0.0\nNaN\nNaN\nE\n24.0\nSE\n...\n53.0\n24.0\n1023.8\n1020.0\nNaN\nNaN\n12.3\n23.3\nNo\nNo\n\n\n145451\n2017-06-17\nUluru\n6.4\n23.4\n0.0\nNaN\nNaN\nESE\n31.0\nS\n...\n53.0\n25.0\n1025.8\n1023.0\nNaN\nNaN\n11.2\n23.1\nNo\nNo\n\n\n145452\n2017-06-18\nUluru\n8.0\n20.7\n0.0\nNaN\nNaN\nESE\n41.0\nSE\n...\n56.0\n32.0\n1028.1\n1024.3\nNaN\n7.0\n11.6\n20.0\nNo\nNo\n\n\n145453\n2017-06-19\nUluru\n7.4\n20.6\n0.0\nNaN\nNaN\nE\n35.0\nESE\n...\n63.0\n33.0\n1027.2\n1023.3\nNaN\nNaN\n11.0\n20.3\nNo\nNo\n\n\n145454\n2017-06-20\nUluru\n3.5\n21.8\n0.0\nNaN\nNaN\nE\n31.0\nESE\n...\n59.0\n27.0\n1024.7\n1021.2\nNaN\nNaN\n9.4\n20.9\nNo\nNo\n\n\n145455\n2017-06-21\nUluru\n2.8\n23.4\n0.0\nNaN\nNaN\nE\n31.0\nSE\n...\n51.0\n24.0\n1024.6\n1020.3\nNaN\nNaN\n10.1\n22.4\nNo\nNo\n\n\n145456\n2017-06-22\nUluru\n3.6\n25.3\n0.0\nNaN\nNaN\nNNW\n22.0\nSE\n...\n56.0\n21.0\n1023.5\n1019.1\nNaN\nNaN\n10.9\n24.5\nNo\nNo\n\n\n145457\n2017-06-23\nUluru\n5.4\n26.9\n0.0\nNaN\nNaN\nN\n37.0\nSE\n...\n53.0\n24.0\n1021.0\n1016.8\nNaN\nNaN\n12.5\n26.1\nNo\nNo\n\n\n145458\n2017-06-24\nUluru\n7.8\n27.0\n0.0\nNaN\nNaN\nSE\n28.0\nSSE\n...\n51.0\n24.0\n1019.4\n1016.5\n3.0\n2.0\n15.1\n26.0\nNo\nNo\n\n\n145459\n2017-06-25\nUluru\n14.9\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\nESE\n...\n62.0\n36.0\n1020.2\n1017.9\n8.0\n8.0\n15.0\n20.9\nNo\nNaN\n\n\n\n\n10 rows × 23 columns\n\n\n\n\n# get the object shape (number of rows, number of columns)\ndf.shape\n\n(145460, 23)\n\n\n\n# get the object length\nlen(df)\n\n145460\n\n\n\n# get all column names\ndf.columns\n\nIndex(['Date', 'Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation',\n       'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm',\n       'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm',\n       'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am',\n       'Temp3pm', 'RainToday', 'RainTomorrow'],\n      dtype='object')\n\n\n\n# get dataframe info (column indices, non-null counts, data types)\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 145460 entries, 0 to 145459\nData columns (total 23 columns):\n #   Column         Non-Null Count   Dtype  \n---  ------         --------------   -----  \n 0   Date           145460 non-null  object \n 1   Location       145460 non-null  object \n 2   MinTemp        143975 non-null  float64\n 3   MaxTemp        144199 non-null  float64\n 4   Rainfall       142199 non-null  float64\n 5   Evaporation    82670 non-null   float64\n 6   Sunshine       75625 non-null   float64\n 7   WindGustDir    135134 non-null  object \n 8   WindGustSpeed  135197 non-null  float64\n 9   WindDir9am     134894 non-null  object \n 10  WindDir3pm     141232 non-null  object \n 11  WindSpeed9am   143693 non-null  float64\n 12  WindSpeed3pm   142398 non-null  float64\n 13  Humidity9am    142806 non-null  float64\n 14  Humidity3pm    140953 non-null  float64\n 15  Pressure9am    130395 non-null  float64\n 16  Pressure3pm    130432 non-null  float64\n 17  Cloud9am       89572 non-null   float64\n 18  Cloud3pm       86102 non-null   float64\n 19  Temp9am        143693 non-null  float64\n 20  Temp3pm        141851 non-null  float64\n 21  RainToday      142199 non-null  object \n 22  RainTomorrow   142193 non-null  object \ndtypes: float64(16), object(7)\nmemory usage: 25.5+ MB\n\n\n\n# calculate the percentage of null values in each column\ndf.isnull().sum()/len(df)\n\nDate             0.000000\nLocation         0.000000\nMinTemp          0.010209\nMaxTemp          0.008669\nRainfall         0.022419\nEvaporation      0.431665\nSunshine         0.480098\nWindGustDir      0.070989\nWindGustSpeed    0.070555\nWindDir9am       0.072639\nWindDir3pm       0.029066\nWindSpeed9am     0.012148\nWindSpeed3pm     0.021050\nHumidity9am      0.018246\nHumidity3pm      0.030984\nPressure9am      0.103568\nPressure3pm      0.103314\nCloud9am         0.384216\nCloud3pm         0.408071\nTemp9am          0.012148\nTemp3pm          0.024811\nRainToday        0.022419\nRainTomorrow     0.022460\ndtype: float64",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#wrangling-data",
    "href": "sessions/03-eda-pandas/index.html#wrangling-data",
    "title": "Exploring Data Using Pandas",
    "section": "Wrangling Data",
    "text": "Wrangling Data\n\nDatasets are rarely perfectly clean and tidy. It is often necessary to make changes to our data, such as selecting a subset of columns, filtering for a range of values, and transforming how certain variables are represented.\nWhat “functions” might we need to carry out on our data when we are exploring it?\nWhat sort of transformations might be necessary here?\n\n\n# selecting a single column by name\ndf['Date']\n\n0         2008-12-01\n1         2008-12-02\n2         2008-12-03\n3         2008-12-04\n4         2008-12-05\n             ...    \n145455    2017-06-21\n145456    2017-06-22\n145457    2017-06-23\n145458    2017-06-24\n145459    2017-06-25\nName: Date, Length: 145460, dtype: object\n\n\n\n# there's always multiple ways to achieve something in python\ndf.loc[:, 'Date'] # this is good\n# df.Date\n\n0         2008-12-01\n1         2008-12-02\n2         2008-12-03\n3         2008-12-04\n4         2008-12-05\n             ...    \n145455    2017-06-21\n145456    2017-06-22\n145457    2017-06-23\n145458    2017-06-24\n145459    2017-06-25\nName: Date, Length: 145460, dtype: object\n\n\n\n# selecting multiple columns (and all rows) by name\ndf[['Date', 'Location', 'Rainfall']]\n# df.loc[:, ['Date', 'Location', 'Rainfall']]\n\n\n\n\n\n\n\n\nDate\nLocation\nRainfall\n\n\n\n\n0\n2008-12-01\nAlbury\n0.6\n\n\n1\n2008-12-02\nAlbury\n0.0\n\n\n2\n2008-12-03\nAlbury\n0.0\n\n\n3\n2008-12-04\nAlbury\n0.0\n\n\n4\n2008-12-05\nAlbury\n1.0\n\n\n...\n...\n...\n...\n\n\n145455\n2017-06-21\nUluru\n0.0\n\n\n145456\n2017-06-22\nUluru\n0.0\n\n\n145457\n2017-06-23\nUluru\n0.0\n\n\n145458\n2017-06-24\nUluru\n0.0\n\n\n145459\n2017-06-25\nUluru\n0.0\n\n\n\n\n145460 rows × 3 columns\n\n\n\n\n# slicing by rows\ndf[200:211]\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n200\n2009-06-19\nAlbury\n0.5\n15.3\n0.0\nNaN\nNaN\nESE\n13.0\nNE\n...\n93.0\n56.0\n1030.8\n1027.1\nNaN\nNaN\n5.4\n15.1\nNo\nNo\n\n\n201\n2009-06-20\nAlbury\n0.9\n17.3\n0.0\nNaN\nNaN\nNNE\n28.0\nNE\n...\n85.0\n56.0\n1025.7\n1020.6\n7.0\n8.0\n7.0\n16.7\nNo\nYes\n\n\n202\n2009-06-21\nAlbury\n7.0\n17.0\n1.6\nNaN\nNaN\nENE\n46.0\nNW\n...\n99.0\n71.0\n1021.9\n1018.6\n1.0\n1.0\n8.3\n16.4\nYes\nYes\n\n\n203\n2009-06-22\nAlbury\n5.0\n14.9\n5.6\nNaN\nNaN\nSE\n19.0\nSE\n...\n99.0\n78.0\n1020.7\n1018.6\n1.0\n8.0\n9.1\n13.6\nYes\nNo\n\n\n204\n2009-06-23\nAlbury\n3.9\n15.5\n0.0\nNaN\nNaN\nW\n35.0\nENE\n...\n99.0\n70.0\n1020.2\n1016.4\n8.0\n6.0\n8.3\n15.2\nNo\nYes\n\n\n205\n2009-06-24\nAlbury\n7.7\n14.1\n6.0\nNaN\nNaN\nESE\n41.0\nESE\n...\n81.0\n65.0\n1014.9\n1012.7\n8.0\n5.0\n9.6\n13.7\nYes\nNo\n\n\n206\n2009-06-25\nAlbury\n4.7\n12.2\n0.0\nNaN\nNaN\nNNW\n24.0\nESE\n...\n99.0\n75.0\n1015.5\n1012.7\n7.0\n8.0\n6.9\n11.6\nNo\nYes\n\n\n207\n2009-06-26\nAlbury\n6.9\n13.7\n4.4\nNaN\nNaN\nSE\n46.0\nESE\n...\n99.0\n73.0\n1011.6\n1008.1\n7.0\nNaN\n8.4\n13.0\nYes\nNo\n\n\n208\n2009-06-27\nAlbury\n8.4\n11.9\n0.0\nNaN\nNaN\nSSE\n22.0\nSE\n...\n79.0\n81.0\n1007.8\n1005.5\n8.0\n8.0\n10.3\n11.3\nNo\nYes\n\n\n209\n2009-06-28\nAlbury\n9.3\n12.3\n5.4\nNaN\nNaN\nW\n22.0\nWSW\n...\n98.0\n76.0\n1007.5\n1006.6\n7.0\n8.0\n9.8\n12.3\nYes\nYes\n\n\n210\n2009-06-29\nAlbury\n8.2\n15.7\n3.6\nNaN\nNaN\nNE\n26.0\nNE\n...\n91.0\n69.0\n1011.4\n1009.4\n8.0\n8.0\n10.1\n15.6\nYes\nYes\n\n\n\n\n11 rows × 23 columns\n\n\n\n\n# filtering by values\n# df[df['Location'] == 'Perth']\n# df[df['Rainfall'] &gt; 0]\ndf[(df['Rainfall'] == 0) & (df['Location'] == 'Perth')]\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n120638\n2008-07-01\nPerth\n2.7\n18.8\n0.0\n0.8\n9.1\nENE\n20.0\nNaN\n...\n97.0\n53.0\n1027.6\n1024.5\n2.0\n3.0\n8.5\n18.1\nNo\nNo\n\n\n120639\n2008-07-02\nPerth\n6.4\n20.7\n0.0\n1.8\n7.0\nNE\n22.0\nESE\n...\n80.0\n39.0\n1024.1\n1019.0\n0.0\n6.0\n11.1\n19.7\nNo\nNo\n\n\n120644\n2008-07-07\nPerth\n0.7\n18.3\n0.0\n0.8\n9.3\nN\n37.0\nNE\n...\n72.0\n36.0\n1028.9\n1024.2\n1.0\n5.0\n8.7\n17.9\nNo\nNo\n\n\n120645\n2008-07-08\nPerth\n3.2\n20.4\n0.0\n1.4\n6.9\nNNW\n24.0\nNE\n...\n58.0\n42.0\n1023.9\n1021.1\n6.0\n5.0\n10.2\n19.3\nNo\nYes\n\n\n120651\n2008-07-14\nPerth\n7.9\n19.7\n0.0\n0.2\n6.5\nNE\n31.0\nNE\n...\n86.0\n41.0\n1026.0\n1021.9\n6.0\n5.0\n11.7\n18.7\nNo\nNo\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n123823\n2017-06-18\nPerth\n7.5\n23.4\n0.0\n1.8\n9.2\nNNE\n28.0\nENE\n...\n67.0\n41.0\n1026.9\n1022.9\n0.0\n0.0\n14.2\n22.2\nNo\nNo\n\n\n123824\n2017-06-19\nPerth\n5.5\n23.0\n0.0\n3.0\n9.1\nSW\n19.0\nENE\n...\n84.0\n55.0\n1023.0\n1020.3\n1.0\n2.0\n11.5\n22.0\nNo\nNo\n\n\n123825\n2017-06-20\nPerth\n7.8\n22.5\n0.0\n2.8\n9.1\nNW\n26.0\nW\n...\n98.0\n59.0\n1019.3\n1015.9\n1.0\n1.0\n13.5\n21.6\nNo\nNo\n\n\n123829\n2017-06-24\nPerth\n11.5\n18.2\n0.0\n3.8\n9.3\nSE\n30.0\nESE\n...\n62.0\n47.0\n1025.9\n1023.4\n1.0\n3.0\n14.0\n17.6\nNo\nNo\n\n\n123830\n2017-06-25\nPerth\n6.3\n17.0\n0.0\n1.6\n7.9\nE\n26.0\nSE\n...\n75.0\n49.0\n1028.6\n1026.0\n1.0\n3.0\n11.5\n15.6\nNo\nNo\n\n\n\n\n2293 rows × 23 columns\n\n\n\n\n# convert date column to datetime\ndf['Date'] = pd.to_datetime(df['Date'])\n\n\n# convert object columns to categorical\ndf.apply(lambda x: x.astype('category') if x.dtype == 'object' else x)\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n0\n2008-12-01\nAlbury\n13.4\n22.9\n0.6\nNaN\nNaN\nW\n44.0\nW\n...\n71.0\n22.0\n1007.7\n1007.1\n8.0\nNaN\n16.9\n21.8\nNo\nNo\n\n\n1\n2008-12-02\nAlbury\n7.4\n25.1\n0.0\nNaN\nNaN\nWNW\n44.0\nNNW\n...\n44.0\n25.0\n1010.6\n1007.8\nNaN\nNaN\n17.2\n24.3\nNo\nNo\n\n\n2\n2008-12-03\nAlbury\n12.9\n25.7\n0.0\nNaN\nNaN\nWSW\n46.0\nW\n...\n38.0\n30.0\n1007.6\n1008.7\nNaN\n2.0\n21.0\n23.2\nNo\nNo\n\n\n3\n2008-12-04\nAlbury\n9.2\n28.0\n0.0\nNaN\nNaN\nNE\n24.0\nSE\n...\n45.0\n16.0\n1017.6\n1012.8\nNaN\nNaN\n18.1\n26.5\nNo\nNo\n\n\n4\n2008-12-05\nAlbury\n17.5\n32.3\n1.0\nNaN\nNaN\nW\n41.0\nENE\n...\n82.0\n33.0\n1010.8\n1006.0\n7.0\n8.0\n17.8\n29.7\nNo\nNo\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n145455\n2017-06-21\nUluru\n2.8\n23.4\n0.0\nNaN\nNaN\nE\n31.0\nSE\n...\n51.0\n24.0\n1024.6\n1020.3\nNaN\nNaN\n10.1\n22.4\nNo\nNo\n\n\n145456\n2017-06-22\nUluru\n3.6\n25.3\n0.0\nNaN\nNaN\nNNW\n22.0\nSE\n...\n56.0\n21.0\n1023.5\n1019.1\nNaN\nNaN\n10.9\n24.5\nNo\nNo\n\n\n145457\n2017-06-23\nUluru\n5.4\n26.9\n0.0\nNaN\nNaN\nN\n37.0\nSE\n...\n53.0\n24.0\n1021.0\n1016.8\nNaN\nNaN\n12.5\n26.1\nNo\nNo\n\n\n145458\n2017-06-24\nUluru\n7.8\n27.0\n0.0\nNaN\nNaN\nSE\n28.0\nSSE\n...\n51.0\n24.0\n1019.4\n1016.5\n3.0\n2.0\n15.1\n26.0\nNo\nNo\n\n\n145459\n2017-06-25\nUluru\n14.9\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\nESE\n...\n62.0\n36.0\n1020.2\n1017.9\n8.0\n8.0\n15.0\n20.9\nNo\nNaN\n\n\n\n\n145460 rows × 23 columns\n\n\n\n\n# filter observations where sunshine is NA\n# (this is illustrative but should not be done without careful consideration generally)\ndf[df['Sunshine'].notnull()]\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n6049\n2009-01-01\nCobar\n17.9\n35.2\n0.0\n12.0\n12.3\nSSW\n48.0\nENE\n...\n20.0\n13.0\n1006.3\n1004.4\n2.0\n5.0\n26.6\n33.4\nNo\nNo\n\n\n6050\n2009-01-02\nCobar\n18.4\n28.9\n0.0\n14.8\n13.0\nS\n37.0\nSSE\n...\n30.0\n8.0\n1012.9\n1012.1\n1.0\n1.0\n20.3\n27.0\nNo\nNo\n\n\n6051\n2009-01-03\nCobar\n15.5\n34.1\n0.0\n12.6\n13.3\nSE\n30.0\nNaN\n...\nNaN\n7.0\nNaN\n1011.6\nNaN\n1.0\nNaN\n32.7\nNo\nNo\n\n\n6052\n2009-01-04\nCobar\n19.4\n37.6\n0.0\n10.8\n10.6\nNNE\n46.0\nNNE\n...\n42.0\n22.0\n1012.3\n1009.2\n1.0\n6.0\n28.7\n34.9\nNo\nNo\n\n\n6053\n2009-01-05\nCobar\n21.9\n38.4\n0.0\n11.4\n12.2\nWNW\n31.0\nWNW\n...\n37.0\n22.0\n1012.7\n1009.1\n1.0\n5.0\n29.1\n35.6\nNo\nNo\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n142298\n2017-06-20\nDarwin\n19.3\n33.4\n0.0\n6.0\n11.0\nENE\n35.0\nSE\n...\n63.0\n32.0\n1013.9\n1010.5\n0.0\n1.0\n24.5\n32.3\nNo\nNo\n\n\n142299\n2017-06-21\nDarwin\n21.2\n32.6\n0.0\n7.6\n8.6\nE\n37.0\nSE\n...\n56.0\n28.0\n1014.6\n1011.2\n7.0\n0.0\n24.8\n32.0\nNo\nNo\n\n\n142300\n2017-06-22\nDarwin\n20.7\n32.8\n0.0\n5.6\n11.0\nE\n33.0\nE\n...\n46.0\n23.0\n1015.3\n1011.8\n0.0\n0.0\n24.8\n32.1\nNo\nNo\n\n\n142301\n2017-06-23\nDarwin\n19.5\n31.8\n0.0\n6.2\n10.6\nESE\n26.0\nSE\n...\n62.0\n58.0\n1014.9\n1010.7\n1.0\n1.0\n24.8\n29.2\nNo\nNo\n\n\n142302\n2017-06-24\nDarwin\n20.2\n31.7\n0.0\n5.6\n10.7\nENE\n30.0\nENE\n...\n73.0\n32.0\n1013.9\n1009.7\n6.0\n5.0\n25.4\n31.0\nNo\nNo\n\n\n\n\n75625 rows × 23 columns",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#summarising-data",
    "href": "sessions/03-eda-pandas/index.html#summarising-data",
    "title": "Exploring Data Using Pandas",
    "section": "Summarising Data",
    "text": "Summarising Data\n\n# quick summary of numeric variables\ndf.describe()\n\n\n\n\n\n\n\n\nDate\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustSpeed\nWindSpeed9am\nWindSpeed3pm\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\n\n\n\n\ncount\n145460\n143975.000000\n144199.000000\n142199.000000\n82670.000000\n75625.000000\n135197.000000\n143693.000000\n142398.000000\n142806.000000\n140953.000000\n130395.00000\n130432.000000\n89572.000000\n86102.000000\n143693.000000\n141851.00000\n\n\nmean\n2013-04-04 21:08:51.907053568\n12.194034\n23.221348\n2.360918\n5.468232\n7.611178\n40.035230\n14.043426\n18.662657\n68.880831\n51.539116\n1017.64994\n1015.255889\n4.447461\n4.509930\n16.990631\n21.68339\n\n\nmin\n2007-11-01 00:00:00\n-8.500000\n-4.800000\n0.000000\n0.000000\n0.000000\n6.000000\n0.000000\n0.000000\n0.000000\n0.000000\n980.50000\n977.100000\n0.000000\n0.000000\n-7.200000\n-5.40000\n\n\n25%\n2011-01-11 00:00:00\n7.600000\n17.900000\n0.000000\n2.600000\n4.800000\n31.000000\n7.000000\n13.000000\n57.000000\n37.000000\n1012.90000\n1010.400000\n1.000000\n2.000000\n12.300000\n16.60000\n\n\n50%\n2013-06-02 00:00:00\n12.000000\n22.600000\n0.000000\n4.800000\n8.400000\n39.000000\n13.000000\n19.000000\n70.000000\n52.000000\n1017.60000\n1015.200000\n5.000000\n5.000000\n16.700000\n21.10000\n\n\n75%\n2015-06-14 00:00:00\n16.900000\n28.200000\n0.800000\n7.400000\n10.600000\n48.000000\n19.000000\n24.000000\n83.000000\n66.000000\n1022.40000\n1020.000000\n7.000000\n7.000000\n21.600000\n26.40000\n\n\nmax\n2017-06-25 00:00:00\n33.900000\n48.100000\n371.000000\n145.000000\n14.500000\n135.000000\n130.000000\n87.000000\n100.000000\n100.000000\n1041.00000\n1039.600000\n9.000000\n9.000000\n40.200000\n46.70000\n\n\nstd\nNaN\n6.398495\n7.119049\n8.478060\n4.193704\n3.785483\n13.607062\n8.915375\n8.809800\n19.029164\n20.795902\n7.10653\n7.037414\n2.887159\n2.720357\n6.488753\n6.93665\n\n\n\n\n\n\n\n\n# a more informative summary function from the skimpy package\nskim(df)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ Dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 145460 │ │ float64     │ 16    │                                                          │\n│ │ Number of columns │ 23     │ │ string      │ 6     │                                                          │\n│ └───────────────────┴────────┘ │ datetime64  │ 1     │                                                          │\n│                                └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━━━┓  │\n│ ┃ column         ┃ NA     ┃ NA %                ┃ mean  ┃ sd    ┃ p0    ┃ p25  ┃ p50  ┃ p75  ┃ p100 ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━━━┩  │\n│ │ MinTemp        │   1485 │  1.0208992162793895 │ 12.19 │ 6.398 │  -8.5 │  7.6 │   12 │ 16.9 │ 33.9 │  ▃▇▇▃  │  │\n│ │ MaxTemp        │   1261 │  0.8669049910628351 │ 23.22 │ 7.119 │  -4.8 │ 17.9 │ 22.6 │ 28.2 │ 48.1 │  ▁▇▇▃  │  │\n│ │ Rainfall       │   3261 │   2.241853430496356 │ 2.361 │ 8.478 │     0 │    0 │    0 │  0.8 │  371 │   ▇    │  │\n│ │ Evaporation    │  62790 │    43.1665062560154 │ 5.468 │ 4.194 │     0 │  2.6 │  4.8 │  7.4 │  145 │   ▇    │  │\n│ │ Sunshine       │  69835 │   48.00976213391998 │ 7.611 │ 3.785 │     0 │  4.8 │  8.4 │ 10.6 │ 14.5 │ ▃▃▅▆▇▃ │  │\n│ │ WindGustSpeed  │  10263 │   7.055547916953114 │ 40.04 │ 13.61 │     6 │   31 │   39 │   48 │  135 │  ▂▇▂   │  │\n│ │ WindSpeed9am   │   1767 │   1.214766946239516 │ 14.04 │ 8.915 │     0 │    7 │   13 │   19 │  130 │   ▇▂   │  │\n│ │ WindSpeed3pm   │   3062 │   2.105046060772721 │ 18.66 │  8.81 │     0 │   13 │   19 │   24 │   87 │  ▅▇▂   │  │\n│ │ Humidity9am    │   2654 │  1.8245565791282827 │ 68.88 │ 19.03 │     0 │   57 │   70 │   83 │  100 │  ▁▂▇▇▆ │  │\n│ │ Humidity3pm    │   4507 │    3.09844630826344 │ 51.54 │  20.8 │     0 │   37 │   52 │   66 │  100 │ ▁▅▆▇▅▂ │  │\n│ │ Pressure9am    │  15065 │     10.356799120033 │  1018 │ 7.107 │ 980.5 │ 1013 │ 1018 │ 1022 │ 1041 │   ▂▇▅  │  │\n│ │ Pressure3pm    │  15028 │  10.331362573903478 │  1015 │ 7.037 │ 977.1 │ 1010 │ 1015 │ 1020 │ 1040 │   ▂▇▅  │  │\n│ │ Cloud9am       │  55888 │   38.42155919153032 │ 4.447 │ 2.887 │     0 │    1 │    5 │    7 │    9 │ ▇▂▃▂▇▅ │  │\n│ │ Cloud3pm       │  59358 │   40.80709473394748 │  4.51 │  2.72 │     0 │    2 │    5 │    7 │    9 │ ▆▂▃▂▇▃ │  │\n│ │ Temp9am        │   1767 │   1.214766946239516 │ 16.99 │ 6.489 │  -7.2 │ 12.3 │ 16.7 │ 21.6 │ 40.2 │  ▂▇▇▃  │  │\n│ │ Temp3pm        │   3609 │  2.4810944589577892 │ 21.68 │ 6.937 │  -5.4 │ 16.6 │ 21.1 │ 26.4 │ 46.7 │  ▁▇▇▃  │  │\n│ └────────────────┴────────┴─────────────────────┴───────┴───────┴───────┴──────┴──────┴──────┴──────┴────────┘  │\n│                                                    datetime                                                     │\n│ ┏━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column         ┃ NA     ┃ NA %       ┃ first                 ┃ last                  ┃ frequency           ┃  │\n│ ┡━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩  │\n│ │ Date           │      0 │          0 │      2007-11-01       │      2017-06-25       │ None                │  │\n│ └────────────────┴────────┴────────────┴───────────────────────┴───────────────────────┴─────────────────────┘  │\n│                                                     string                                                      │\n│ ┏━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┓  │\n│ ┃           ┃       ┃          ┃          ┃           ┃          ┃         ┃ chars    ┃ words per ┃ total    ┃  │\n│ ┃ column    ┃ NA    ┃ NA %     ┃ shortest ┃ longest   ┃ min      ┃ max     ┃ per row  ┃ row       ┃ words    ┃  │\n│ ┡━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━┩  │\n│ │ Location  │     0 │        0 │ Sale     │ Melbourne │ Adelaide │ Woomera │     8.71 │         1 │   145460 │  │\n│ │           │       │          │          │ Airport   │          │         │          │           │          │  │\n│ │ WindGustD │ 10326 │ 7.098858 │ W        │ WNW       │ E        │ WSW     │     2.19 │      0.93 │   135134 │  │\n│ │ ir        │       │ 79279527 │          │           │          │         │          │           │          │  │\n│ │ WindDir9a │ 10566 │ 7.263852 │ W        │ NNW       │ E        │ WSW     │     2.18 │      0.93 │   134894 │  │\n│ │ m         │       │ 60552729 │          │           │          │         │          │           │          │  │\n│ │           │       │        2 │          │           │          │         │          │           │          │  │\n│ │ WindDir3p │  4228 │ 2.906641 │ E        │ WNW       │ E        │ WSW     │     2.21 │      0.97 │   141232 │  │\n│ │ m         │       │ 00096246 │          │           │          │         │          │           │          │  │\n│ │           │       │        4 │          │           │          │         │          │           │          │  │\n│ │ RainToday │  3261 │ 2.241853 │ No       │ Yes       │ No       │ Yes     │     2.22 │      0.98 │   142199 │  │\n│ │           │       │ 43049635 │          │           │          │         │          │           │          │  │\n│ │           │       │        6 │          │           │          │         │          │           │          │  │\n│ │ RainTomor │  3267 │ 2.245978 │ No       │ Yes       │ No       │ Yes     │     2.22 │      0.98 │   142193 │  │\n│ │ row       │       │ 27581465 │          │           │          │         │          │           │          │  │\n│ │           │       │        7 │          │           │          │         │          │           │          │  │\n│ └───────────┴───────┴──────────┴──────────┴───────────┴──────────┴─────────┴──────────┴───────────┴──────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\n\n# count unique values\ndf['Location'].nunique()\n\n49\n\n\n\n# get unique values\ndf['Location'].unique()\n\narray(['Albury', 'BadgerysCreek', 'Cobar', 'CoffsHarbour', 'Moree',\n       'Newcastle', 'NorahHead', 'NorfolkIsland', 'Penrith', 'Richmond',\n       'Sydney', 'SydneyAirport', 'WaggaWagga', 'Williamtown',\n       'Wollongong', 'Canberra', 'Tuggeranong', 'MountGinini', 'Ballarat',\n       'Bendigo', 'Sale', 'MelbourneAirport', 'Melbourne', 'Mildura',\n       'Nhil', 'Portland', 'Watsonia', 'Dartmoor', 'Brisbane', 'Cairns',\n       'GoldCoast', 'Townsville', 'Adelaide', 'MountGambier', 'Nuriootpa',\n       'Woomera', 'Albany', 'Witchcliffe', 'PearceRAAF', 'PerthAirport',\n       'Perth', 'SalmonGums', 'Walpole', 'Hobart', 'Launceston',\n       'AliceSprings', 'Darwin', 'Katherine', 'Uluru'], dtype=object)\n\n\n\n# calculate variable mean\nnp.round(df['Sunshine'].mean(), decimals=2)\n\nnp.float64(7.61)\n\n\n\n# calculate other summary statistics\ndf['Sunshine'].median()\n# df['Sunshine'].sum()\n\nnp.float64(8.4)\n\n\n\n# calculate group means\nnp.round(df.groupby(by='Location')['Sunshine'].mean(), decimals=1)\n\nLocation\nAdelaide            7.7\nAlbany              6.7\nAlbury              NaN\nAliceSprings        9.6\nBadgerysCreek       NaN\nBallarat            NaN\nBendigo             NaN\nBrisbane            8.1\nCairns              7.6\nCanberra            7.4\nCobar               8.7\nCoffsHarbour        7.4\nDartmoor            6.5\nDarwin              8.5\nGoldCoast           NaN\nHobart              6.6\nKatherine           NaN\nLaunceston          NaN\nMelbourne           6.4\nMelbourneAirport    6.4\nMildura             8.5\nMoree               8.9\nMountGambier        6.5\nMountGinini         NaN\nNewcastle           NaN\nNhil                NaN\nNorahHead           NaN\nNorfolkIsland       7.0\nNuriootpa           7.7\nPearceRAAF          8.8\nPenrith             NaN\nPerth               8.8\nPerthAirport        8.8\nPortland            6.5\nRichmond            NaN\nSale                6.7\nSalmonGums          NaN\nSydney              7.2\nSydneyAirport       7.2\nTownsville          8.5\nTuggeranong         NaN\nUluru               NaN\nWaggaWagga          8.2\nWalpole             NaN\nWatsonia            6.4\nWilliamtown         7.2\nWitchcliffe         NaN\nWollongong          NaN\nWoomera             9.0\nName: Sunshine, dtype: float64\n\n\n\n# group by location and count non-null sunshine values\ndf.groupby('Location')['Sunshine'].count()\n\nLocation\nAdelaide            1769\nAlbany              2520\nAlbury                 0\nAliceSprings        2520\nBadgerysCreek          0\nBallarat               0\nBendigo                0\nBrisbane            3144\nCairns              2564\nCanberra            1521\nCobar                550\nCoffsHarbour        1494\nDartmoor            2566\nDarwin              3189\nGoldCoast              0\nHobart              3179\nKatherine              0\nLaunceston             0\nMelbourne           3192\nMelbourneAirport    3008\nMildura             2876\nMoree               2055\nMountGambier        2597\nMountGinini            0\nNewcastle              0\nNhil                   0\nNorahHead              0\nNorfolkIsland       2570\nNuriootpa           2848\nPearceRAAF          3004\nPenrith                0\nPerth               3188\nPerthAirport        3004\nPortland            2566\nRichmond               0\nSale                1818\nSalmonGums             0\nSydney              3328\nSydneyAirport       2993\nTownsville          2617\nTuggeranong            0\nUluru                  0\nWaggaWagga          2575\nWalpole                0\nWatsonia            3008\nWilliamtown         1355\nWitchcliffe            0\nWollongong             0\nWoomera             2007\nName: Sunshine, dtype: int64",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#exercises",
    "href": "sessions/03-eda-pandas/index.html#exercises",
    "title": "Exploring Data Using Pandas",
    "section": "Exercises",
    "text": "Exercises\nSome of these questions are easily answered by scrolling up and finding the answer in the output of the above code, however, the goal is to find the answer using code. No one actually cares what the answer to any of these questions is, it’s the process that matters!\nRemember, if you don’t know the answer, it’s okay to Google it (or speak to others, including me, for help)!\n\nWhat is the ‘Sunshine’ column’s data type?\nIdentify all the columns that are of dtype ‘object’.\nHow many of the dataframe’s columns are of dtype ‘object’?\nHow many of the ‘Rainfall’ column values are NAs?\nCreate a new dataframe which only includes the ‘Date’, ‘Location, ’Sunshine’, ‘Rainfall’, and ‘RainTomorrow’ columns.\nConvert ‘RainTomorrow’ to a numeric variable, where ‘Yes’ = 1 and ‘No’ = 0.\nWhat is the average amount of rainfall for each location?\nWhat is the average amount of rainfall for days that it will rain tomorrow?\nWhat is the average amount of sunshine in Perth when it will not rain tomorrow?\nWe want to understand the role that time plays in the dataset. Using the original dataframe, carry the following tasks and answer the corresponding questions:\n\nCreate columns representing the year and month from the ‘Date’ column. How many years of data are in the dataset?\nExamine the distribution of the ‘Sunshine’ NAs over time. Is time a component in the ‘Sunshine’ data quality issues?\nCalculate the average rainfall and sunshine by month. How do rainfall and sunshine vary through the year?\nCalculate the average rainfall and sunshine by year. How have rainfall and sunshine changed over time?\n\n\n\nSolutions\n\n# import packages\nimport numpy as np\nimport pandas as pd\n\n\n# import the dataset\ndf = pd.read_csv('data/weatherAUS.csv')\n\n\nWhat is the ‘Sunshine’ column’s data type?\n\n\n# What is the 'Sunshine' column's data type?\ndf['Sunshine'].dtypes\n\ndtype('float64')\n\n\n\nIdentify all the columns that are of dtype ‘object’.\n\n\n# Identify all the columns that are of dtype 'object'\nlist(df.select_dtypes(include=['object']))\n\n['Date',\n 'Location',\n 'WindGustDir',\n 'WindDir9am',\n 'WindDir3pm',\n 'RainToday',\n 'RainTomorrow']\n\n\n\nHow many of the dataframe’s columns are of dtype ‘object’?\n\n\n# How many of the dataframe's columns are of dtype 'object'?\nlen(list(df.select_dtypes(include=['object'])))\n\n7\n\n\n\nHow many of the ‘Rainfall’ column values are NAs?\n\n\n# How many of the 'Rainfall' column values are NAs?\ndf['Rainfall'].isna().sum()\n\nnp.int64(3261)\n\n\n\nCreate a new dataframe which only includes the ‘Date’, ‘Location, ’Sunshine’, ‘Rainfall’, and ‘RainTomorrow’ columns.\n\n\n# Create a new dataframe which only includes the 'Date', 'Location, 'Sunshine', 'Rainfall', and 'RainTomorrow' columns.\nnew_df = df[['Date', 'Location', 'Sunshine', 'Rainfall', 'RainTomorrow']]\nnew_df.head()\n\n\n\n\n\n\n\n\nDate\nLocation\nSunshine\nRainfall\nRainTomorrow\n\n\n\n\n0\n2008-12-01\nAlbury\nNaN\n0.6\nNo\n\n\n1\n2008-12-02\nAlbury\nNaN\n0.0\nNo\n\n\n2\n2008-12-03\nAlbury\nNaN\n0.0\nNo\n\n\n3\n2008-12-04\nAlbury\nNaN\n0.0\nNo\n\n\n4\n2008-12-05\nAlbury\nNaN\n1.0\nNo\n\n\n\n\n\n\n\n\nConvert ‘RainTomorrow’ to a numeric variable, where ‘Yes’ = 1 and ‘No’ = 0.\n\n\n# Convert 'RainTomorrow' to a numeric variable, where 'Yes' = 1 and 'No' = 0.\n# df['Location'].astype('category').cat.codes\n# df['RainTomorrow'].astype('category').cat.codes\ndf['RainTomorrow'].map({'Yes': 1, 'No': 0})\n\n0         0.0\n1         0.0\n2         0.0\n3         0.0\n4         0.0\n         ... \n145455    0.0\n145456    0.0\n145457    0.0\n145458    0.0\n145459    NaN\nName: RainTomorrow, Length: 145460, dtype: float64\n\n\n\nWhat is the average amount of rainfall for each location?\n\n\n# What is the average amount of rainfall for each location?\ndf.groupby('Location')['Rainfall'].mean().sort_values(ascending=False)\n\nLocation\nCairns              5.742035\nDarwin              5.092452\nCoffsHarbour        5.061497\nGoldCoast           3.769396\nWollongong          3.594903\nWilliamtown         3.591108\nTownsville          3.485592\nNorahHead           3.387299\nSydney              3.324543\nMountGinini         3.292260\nKatherine           3.201090\nNewcastle           3.183892\nBrisbane            3.144891\nNorfolkIsland       3.127665\nSydneyAirport       3.009917\nWalpole             2.906846\nWitchcliffe         2.895664\nPortland            2.530374\nAlbany              2.263859\nBadgerysCreek       2.193101\nPenrith             2.175304\nTuggeranong         2.164043\nDartmoor            2.146567\nRichmond            2.138462\nMountGambier        2.087562\nLaunceston          2.011988\nAlbury              1.914115\nPerth               1.906295\nMelbourne           1.870062\nWatsonia            1.860820\nPerthAirport        1.761648\nCanberra            1.741720\nBallarat            1.740026\nWaggaWagga          1.709946\nPearceRAAF          1.669080\nMoree               1.630203\nBendigo             1.619380\nHobart              1.601819\nAdelaide            1.566354\nSale                1.510167\nMelbourneAirport    1.451977\nNuriootpa           1.390343\nCobar               1.127309\nSalmonGums          1.034382\nMildura             0.945062\nNhil                0.934863\nAliceSprings        0.882850\nUluru               0.784363\nWoomera             0.490405\nName: Rainfall, dtype: float64\n\n\n\nWhat is the average amount of rainfall for days that it will rain tomorrow?\n\n\n# What is the average amount of rainfall for days that it will rain tomorrow?\ndf.groupby('RainTomorrow')['Rainfall'].mean()\n\nRainTomorrow\nNo     1.270290\nYes    6.142104\nName: Rainfall, dtype: float64\n\n\n\nWhat is the average amount of sunshine in Perth when it will not rain tomorrow?\n\n\n# What is the average amount of sunshine in Perth when it will not rain tomorrow?\ndf.loc[(df['Location'] == 'Perth') & (df['RainTomorrow'] == 'No'), 'Sunshine'].mean()\n# df[(df['Location']=='Perth') & (df['RainTomorrow']=='No')]['Sunshine'].mean()\n\nnp.float64(9.705306603773584)\n\n\n\nWe want to understand the role that time plays in the dataset. Using the original dataframe, carry the following tasks and answer the corresponding questions:\n\nCreate columns representing the year and month from the ‘Date’ column. How many years of data are in the dataset?\nExamine the distribution of the ‘Sunshine’ NAs over time. Is time a component in the ‘Sunshine’ data quality issues?\nCalculate the average rainfall and sunshine by month. How do rainfall and sunshine vary through the year?\nCalculate the average rainfall and sunshine by year. How have rainfall and sunshine changed over time?\n\n\n\n# Create columns representing the year and month from the 'Date' column. How many years of data are in the dataset?\ndf = (\n    df.assign(Date=pd.to_datetime(df['Date']))\n    .assign(\n        Year=lambda x: x['Date'].dt.year,\n        Month=lambda x: x['Date'].dt.month\n    )\n)\n\ndf['Year'].nunique()\n\n11\n\n\n\n# Examine the distribution of the 'Sunshine' NAs over time. Is time a component in the 'Sunshine' data quality issues?\ndf.groupby('Year')['Sunshine'].apply(lambda x: x.isna().sum())\n\nYear\n2007        0\n2008      323\n2009     6146\n2010     6220\n2011     6053\n2012     6539\n2013     7570\n2014     9157\n2015     9441\n2016    11994\n2017     6392\nName: Sunshine, dtype: int64\n\n\n\n# Calculate the average rainfall and sunshine by month. How do rainfall and sunshine vary through the year?\ndf.groupby('Month')[['Rainfall', 'Sunshine']].mean()\n\n\n\n\n\n\n\n\nRainfall\nSunshine\n\n\nMonth\n\n\n\n\n\n\n1\n2.738478\n9.203301\n\n\n2\n3.188665\n8.590477\n\n\n3\n2.814450\n7.627229\n\n\n4\n2.334823\n7.080942\n\n\n5\n2.001654\n6.321179\n\n\n6\n2.782182\n5.645576\n\n\n7\n2.184209\n6.052353\n\n\n8\n2.034983\n7.136923\n\n\n9\n1.888543\n7.685301\n\n\n10\n1.614732\n8.490807\n\n\n11\n2.268177\n8.666098\n\n\n12\n2.491835\n8.997794\n\n\n\n\n\n\n\n\n# Calculate the average rainfall and sunshine by year. How have rainfall and sunshine changed over time?\ndf.groupby('Year')[['Rainfall', 'Sunshine']].mean()\n\n\n\n\n\n\n\n\nRainfall\nSunshine\n\n\nYear\n\n\n\n\n\n\n2007\n3.219672\n8.086885\n\n\n2008\n2.293541\n7.789111\n\n\n2009\n2.166385\n7.905102\n\n\n2010\n2.710924\n7.277599\n\n\n2011\n2.829197\n7.313705\n\n\n2012\n2.416200\n7.580733\n\n\n2013\n2.272402\n7.663279\n\n\n2014\n1.966341\n7.793905\n\n\n2015\n2.160753\n7.689140\n\n\n2016\n2.384054\n7.646902\n\n\n2017\n2.478834\n7.676602",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/05-eda-seaborn/index.html",
    "href": "sessions/05-eda-seaborn/index.html",
    "title": "Visually Exploring Data Using Seaborn",
    "section": "",
    "text": "We are using Australian weather data, taken from Kaggle. To download the data, click here.\n# import packages\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# import the dataset\ndf = pd.read_csv('data/weatherAUS.csv')\nObjectives:\n# subset of observations from five biggest cities\nbig_cities = (\n    df.loc[df['Location'].isin(['Adelaide', 'Brisbane', 'Melbourne', 'Perth', 'Sydney'])]\n    .copy()\n)",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "5. EDA with Seaborn"
    ]
  },
  {
    "objectID": "sessions/05-eda-seaborn/index.html#visualising-a-single-variable",
    "href": "sessions/05-eda-seaborn/index.html#visualising-a-single-variable",
    "title": "Visually Exploring Data Using Seaborn",
    "section": "Visualising a Single Variable",
    "text": "Visualising a Single Variable\n\nWhat do we want to know when we are visualising a sample taken from a single variable?\nWe want to understand the value that the value tends to take, and how much it tends to deviate from its typical value.\n\nThe central tendency and deviation are ways to describe a sample.\nVisualising the distribution of a variable can tell us these things (approximately), and can tell us about the shape of the data too.\n\n\n\nDescribing a Sample\n\nWhat is the best way to describe a variable?\n\nWhat is the average value? Or the value it is most likely to take? What is the best value to describe it in one go?\n\nThe “central tendency” is the average or most common value that a variable takes. Mean, median, and mode are all descriptions of the central tendency.\n\nMean - Sum of values in a sample divided by the total number of observations\nMedian - The midpoint value if the sample is ordered from highest to lowest\nMode - The most common value in the sample\n\nThe mean is the most common approach, but the mean, median, and mode choice are context-dependent. Other approaches exist, too, such as the geometric mean.\n\nThe geometric mean multiplies all values in the sample and takes the \\(n\\)th root of that multiplied value.\nIt can be useful when dealing with skewed data or data with very large ranges, and when dealing with rates, proportions etc. However it can’t handle zeros or negative values.\n\nThe mode value is generally most useful when dealing with categorical variables.\n\n\n# mode rainfall by location\nbig_cities.groupby('Location')['Rainfall'].agg(pd.Series.mode)\n\nLocation\nAdelaide     0.0\nBrisbane     0.0\nMelbourne    0.0\nPerth        0.0\nSydney       0.0\nName: Rainfall, dtype: float64\n\n\n\n# mode location\nbig_cities['Location'].agg(pd.Series.mode)\n\n0    Sydney\nName: Location, dtype: object\n\n\n\n# mode location using value counts\nbig_cities['Location'].value_counts().iloc[0:1]\n\nLocation\nSydney    3344\nName: count, dtype: int64\n\n\n\n# mean rainfall by location\nnp.round(big_cities.groupby('Location')['Rainfall'].mean(), decimals=2)\n\nLocation\nAdelaide     1.57\nBrisbane     3.14\nMelbourne    1.87\nPerth        1.91\nSydney       3.32\nName: Rainfall, dtype: float64\n\n\n\n# median rainfall by location\nbig_cities.groupby('Location')['Rainfall'].median()\n\nLocation\nAdelaide     0.0\nBrisbane     0.0\nMelbourne    0.0\nPerth        0.0\nSydney       0.0\nName: Rainfall, dtype: float64\n\n\n\n# geometric mean max temperature by location\nbig_cities.groupby('Location')['MaxTemp'].apply(lambda x: np.exp(np.log(x).mean()))\n\nLocation\nAdelaide     21.888697\nBrisbane     26.152034\nMelbourne    19.972352\nPerth        24.320203\nSydney       22.570993\nName: MaxTemp, dtype: float64\n\n\n\nWhy do the mean and median differ so much? Why would the median rainfall be zero for all five cities?\nDoes this matter? How would it change our understanding of the rainfall variable?\nVisualising the distribution can tell us more!\n\n\n\nComparing the Mean & Median\n\nWe have simulated three different distributions that have slightly different shapes. and see how their mean and median values differ.\n\n\n# generate distributions\nnp.random.seed(123)\nnormal_dist = np.random.normal(10, 1, 1000)\nright_skewed_dist = np.concatenate([np.random.normal(8, 2, 600), np.random.normal(14, 4, 400)])\nleft_skewed_dist = np.concatenate([np.random.normal(14, 2, 600), np.random.normal(8, 4, 400)])\n\n\n# import packages\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# control some deprecation warnings in seaborn\nwarnings.filterwarnings(\n    \"ignore\",\n    category=FutureWarning,\n    module=\"seaborn\"\n)\n\n# set figure size\nplt.rcParams['figure.figsize'] = (12, 6)\n\n# function for calculating summary statistics and plotting distributions\ndef plot_averages(ax, data, title):\n    mean = np.mean(data)\n    median = np.median(data)\n    \n    sns.histplot(data, color=\"#d9dcd6\", bins=30, ax=ax)\n    ax.axvline(mean, color=\"#0081a7\", linewidth=3, linestyle=\"--\", label=f\"Mean: {mean:.2f}\")\n    ax.axvline(median, color=\"#ef233c\", linewidth=3, linestyle=\"--\", label=f\"Median: {median:.2f}\")\n    ax.set_title(title)\n    ax.set_ylabel('')\n    ax.legend()\n\n\n# plot distributions\nfig, axes = plt.subplots(1, 3, sharey=True)\n\nplot_averages(axes[0], normal_dist, \"Normal Distribution\\n(Mean ≈ Median)\")\nplot_averages(axes[1], right_skewed_dist, \"Right-Skewed Distribution\\n(Mean &gt; Median)\")\nplot_averages(axes[2], left_skewed_dist, \"Left-Skewed Distribution\\n(Mean &lt; Median)\")\n\nplt.suptitle(\"Comparison of Mean & Median Across Distributions\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe mean and median of the normal distribution are identical, while the two skewed distributions have slightly different means and medians.\n\nThe mean is larger than the media when the distribution is right-skewed, and the median is larger than the mean when it is left-skewed.\nWhen the distribution is skewed, the median value will be a better description of the central tendency, because the mean value is more sensitive to extreme values (and skewed distributions have longer tails of extreme values).\n\nThese differences point to another important factor to consider when summarising data - the spread or deviation of the sample.\nHow do we measure how a sample is spread around the central tendency?\n\nStandard deviation and variance quantify spread.\nVariance, the average squared difference between observations and the mean value, measures how spread out a sample is.\nStandard deviation is the square root of the variance. It’s easier to interpret because it’s in the same units as the sample.\n\n\n\n# generate distributions\nnp.random.seed(123)\nmean = 10\nstd_devs = [1, 2, 3]\ndistributions = [np.random.normal(mean, std_dev, 1000) for std_dev in std_devs]\n\n\n# function for calculating summary statistics and plotting distributions\ndef plot_spread(ax, data, std_dev, title):\n    mean = np.mean(data)\n    std_dev = np.std(data)\n\n    sns.histplot(data, color=\"#d9dcd6\", bins=30, ax=ax)\n    ax.axvline(mean, color=\"#0081a7\", linewidth=3, linestyle=\"--\", label=f\"Mean: {mean:.2f}\")\n    ax.axvline(mean + std_dev, color=\"#ee9b00\", linewidth=3, linestyle=\"--\", label=f\"Mean + 1 SD: {mean + std_dev:.2f}\")\n    ax.axvline(mean - std_dev, color=\"#ee9b00\", linewidth=3, linestyle=\"--\", label=f\"Mean - 1 SD: {mean - std_dev:.2f}\")\n    ax.set_title(f\"{title}\")\n    ax.legend()\n\n\n# plot distributions\nfig, axes = plt.subplots(1, 3, sharey=True, sharex=True)\n\nfor i, std_dev in enumerate(std_devs):\n    plot_spread(axes[i], distributions[i], std_dev, f\"Standard Deviation = {std_dev}\")\n\nplt.suptitle(\"Effect of Standard Deviation on Distribution Shape\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAs standard deviation increases, the spread of values around the mean increases.\nWe can compute various summary statistics that describe a sample (mean, median, standard deviation, kurtosis etc. etc.), or we can just visualise it!\nVisualising distributions is a good starting point for understanding a sample. It can quickly and easily tell you a lot about the data.\n\n\n# plot distribution of rainfall\nrainfall_mean = np.mean(big_cities['Rainfall'])\nrainfall_median = np.median(big_cities['Rainfall'].dropna())\n\nsns.histplot(data=big_cities, x='Rainfall', binwidth=10, color=\"#d9dcd6\")\nplt.axvline(rainfall_mean, color=\"#0081a7\", linestyle=\"--\", linewidth=2, label=f\"Mean: {rainfall_mean:.2f}\")\nplt.axvline(rainfall_median, color=\"#ef233c\", linestyle=\"--\", linewidth=2, label=f\"Median: {rainfall_median:.2f}\")\n\nplt.title(\"Distribution of Rainfall in Australia's Big Cities\")\n\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# plot distribution of sunshine\nsunshine_mean = np.mean(big_cities['Sunshine'])\nsunshine_median = np.median(big_cities['Sunshine'].dropna())\n\nsns.histplot(data=big_cities, x='Sunshine', binwidth=1, color=\"#d9dcd6\")\nplt.axvline(sunshine_mean, color=\"#0081a7\", linestyle=\"--\", linewidth=2, label=f\"Mean: {sunshine_mean:.2f}\")\nplt.axvline(sunshine_median, color=\"#ef233c\", linestyle=\"--\", linewidth=2, label=f\"Median: {sunshine_median:.2f}\")\n\nplt.title(\"Distribution of Sunshine in Australia's Big Cities\")\n\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThese two plots require a little more code, but we can get most of what we want with a lot less.\n\n\nsns.histplot(data=big_cities, x='MaxTemp')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.countplot(big_cities, x='Location', color=\"#d9dcd6\", edgecolor='black')\nplt.ylim(3000, 3500)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.countplot(big_cities, x='RainTomorrow', color=\"#d9dcd6\", edgecolor='black')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nbig_cities['RainTomorrow'].value_counts()\n\nRainTomorrow\nNo     11673\nYes     3543\nName: count, dtype: int64",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "5. EDA with Seaborn"
    ]
  },
  {
    "objectID": "sessions/05-eda-seaborn/index.html#visualising-multiple-variables",
    "href": "sessions/05-eda-seaborn/index.html#visualising-multiple-variables",
    "title": "Visually Exploring Data Using Seaborn",
    "section": "Visualising Multiple Variables",
    "text": "Visualising Multiple Variables\n\nWe will often want to know how values of a given variable change based on the values of another.\nThis may not indicate a relationship, but it helps us better understand our data.\n\n\nsns.barplot(big_cities, x='Location', y='Sunshine', color=\"#d9dcd6\", edgecolor='black')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.boxplot(big_cities, x='Location', y='MaxTemp', color=\"#d9dcd6\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.boxplot(data=big_cities, x='RainTomorrow', y='Humidity3pm', color=\"#d9dcd6\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.kdeplot(data=big_cities, x='Humidity3pm', hue='RainTomorrow')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n(\n    big_cities\n    # convert date to datetime\n    .assign(Date=pd.to_datetime(big_cities['Date']))\n    # create year-month column\n    .assign(Year_Month=lambda x: x['Date'].dt.to_period('M'))\n    # group by year-month and calculate sum of rainfall\n    .groupby('Year_Month')['Rainfall'].sum()\n    # convert year-month index back to column in dataframe\n    .reset_index()\n    # create year-month timestamp for plotting\n    .assign(Year_Month=lambda x: x['Year_Month'].dt.to_timestamp()) \n    # pass df object to seaborn lineplot\n    .pipe(lambda df: sns.lineplot(data=df, x='Year_Month', y='Rainfall', linewidth=2))\n)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n(\n    big_cities\n    # convert date to datetime object\n    .assign(Date=pd.to_datetime(big_cities['Date']))\n    # set date column as index\n    .set_index('Date')\n    # resample by month-end for monthly aggregations\n    .resample('ME')\n    # calculate mean sunshine per month\n    .agg({'Sunshine': 'mean'})\n    # convert month index back to column in dataframe\n    .reset_index()\n    # pass df object to seaborn lineplot\n    .pipe(lambda df: sns.lineplot(data=df, x='Date', y='Sunshine', color=\"#1f77b4\", linewidth=2))\n)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(1, 2)\n\n(\n    big_cities\n    .assign(Date=pd.to_datetime(big_cities['Date']))\n    .assign(Month=lambda x: x['Date'].dt.month)\n    .groupby('Month')['Rainfall'].mean()\n    .reset_index()\n    .pipe(lambda df: sns.lineplot(data=df, x='Month', y='Rainfall', color=\"#1f77b4\", linewidth=2, ax=axes[0]))\n)\n\n(\n    big_cities\n    .assign(Date=pd.to_datetime(big_cities['Date']))\n    .assign(Month=lambda x: x['Date'].dt.month) \n    .groupby('Month')['Sunshine'].mean() \n    .reset_index()\n    .pipe(lambda df: sns.lineplot(data=df, x='Month', y='Sunshine', color=\"#ff7f0e\", linewidth=2, ax=axes[1]))\n)\n\nxticks = range(1, 13)\nxticklabels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\nfor ax in axes:\n    ax.set_xticks(xticks)  # Set ticks\n    ax.set_xticklabels(xticklabels, rotation=45)\n    ax.set_xlabel('')\n    ax.set_ylabel('')\naxes[0].set_title('Average Rainfall by Month', fontsize=16)\naxes[1].set_title('Average Sunshine by Month', fontsize=16)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "5. EDA with Seaborn"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#what-is-a-notebook",
    "href": "sessions/02-jupyter_notebooks/slides.html#what-is-a-notebook",
    "title": "Jupyter Notebooks",
    "section": "What is a notebook",
    "text": "What is a notebook\n\nThe standard for programming in python is the .py file which can hold a block of code which can contain lines of code that allow you to export the results as visualisations or data files.\nJupyter Notebooks have been developed with the data science and analytical community.\nNotebooks are a collection interactive cells which a user can run as a collection or individually, based on the current state of program.\nCells can be denoted as Code, Markdown or Raw Depending on use case.\n\nCode cells use a process called a kernel to run programme elements in the user selected code base (e.g. Python or R).\nMarkdown cells allow the user to include formatted text and other elements (such as links and images).\nRaw cells have no processing attached and output as plain text."
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#a-brief-history-of-jupyter-notebooks",
    "href": "sessions/02-jupyter_notebooks/slides.html#a-brief-history-of-jupyter-notebooks",
    "title": "Jupyter Notebooks",
    "section": "A brief history of Jupyter notebooks",
    "text": "A brief history of Jupyter notebooks\n\nIn 2001, Fernando Perez started development of the iPython project as a way of incorporating prompts and access to previous output, as he continued development he amalgamated iPython with 2 other projects\nIn 2014, Project Jupyter was born out of the initial iPython project. The key aim was to make the project independent of a programming language and allow different code bases to use notebooks. The Name is a reference to the three initial languages: Julia, Python, and R.\nJupyter Notebooks and more recently Jupiter Labs are more than just the notebook, they are interactive development environments launched from the command line.\nJupyter notebooks are used by many online platforms and service providers including: Kaggle, Microsoft Fabric, and the NHS Federated Data Platform."
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#pros-and-cons-of-using-a-notebook",
    "href": "sessions/02-jupyter_notebooks/slides.html#pros-and-cons-of-using-a-notebook",
    "title": "Jupyter Notebooks",
    "section": "Pros and cons of using a notebook",
    "text": "Pros and cons of using a notebook\nOn the plus side…\n\nNotebooks are highly interactive and allow cells to be run in any order.\nYou can re-run each cell separately, so iterative testing is more granular.\nNotebooks can be used to provide a structured report for an end user regardless of coding knowledge.\n\nHaving said that…\n\nIf you are not careful you can save a notebook in a state that cannot run as intended if changes are not checked.\nIt can be harder to understand complex code interactions."
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#the-toolkit",
    "href": "sessions/02-jupyter_notebooks/slides.html#the-toolkit",
    "title": "Jupyter Notebooks",
    "section": "The Toolkit",
    "text": "The Toolkit\n\nYou will need the following pre-installed:\n\nLanguage: Python\nDependency management: uv\nVersion Control: Git, GitHub Desktop\nIDE: VS Code (or your preferred IDE)\n\nYou can install all these tools by running the following in PowerShell:\n\nwinget install astral-sh.uv Microsoft.VisualStudioCode github-desktop"
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#walkthrough-and-demonstration",
    "href": "sessions/02-jupyter_notebooks/slides.html#walkthrough-and-demonstration",
    "title": "Jupyter Notebooks",
    "section": "Walkthrough and demonstration",
    "text": "Walkthrough and demonstration\nif reviewing these slides this section is only available in the recording, though the initial steps used should be available on the associated Code Club site page"
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#resources",
    "href": "sessions/02-jupyter_notebooks/slides.html#resources",
    "title": "Jupyter Notebooks",
    "section": "Resources",
    "text": "Resources\n\nCheck out the History of iPython\nYou can find out more about Project Jupyter\nThe demonstration makes use of this markdown cheatsheet\nLikewise this is the Jupyter shortcuts Cheat Sheet"
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#what-to-expect",
    "href": "sessions/01-onboarding/slides.html#what-to-expect",
    "title": "Python Onboarding",
    "section": "What to Expect?",
    "text": "What to Expect?\n\nLearning a language is hard. It can be frustrating. Perseverance is key to success.\nThese sessions will introduce you to Python, showing you what is possible and how to achieve some of what might benefit your work.\nBut the real learning comes by doing. You need to run the code yourself, have a play around, and cement what you’ve learned by applying it.\nPractice, repetition, and making mistakes along the way is how real progress is made."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#why-learn-python",
    "href": "sessions/01-onboarding/slides.html#why-learn-python",
    "title": "Python Onboarding",
    "section": "Why Learn Python?",
    "text": "Why Learn Python?\n\nCoding skills, generally, and Python specifically, seem to be a priority in the NHS right now. It’s a clear direction of travel. Learning now sets you up for the future.\nPython and the applied skills taught in these sessions will enable you to use advanced methods and design flexible, scalable solutions.\nPython is very valuable for career development.\nIt is (hopefully) fun!"
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#the-toolkit",
    "href": "sessions/01-onboarding/slides.html#the-toolkit",
    "title": "Python Onboarding",
    "section": "The Toolkit",
    "text": "The Toolkit\n\nWe will be using the following tools throughout this course:\n\nLanguage: Python\nDependency management: uv\nVersion Control: Git, GitHub Desktop\nIDE: VS Code/Jupyter Notebooks (or your preferred IDE)\n\nYou can install all these tools by running the following in PowerShell:\n\nwinget install astral-sh.uv Microsoft.VisualStudioCode github-desktop"
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#python",
    "href": "sessions/01-onboarding/slides.html#python",
    "title": "Python Onboarding",
    "section": "Python",
    "text": "Python\n\nPython is an all-purpose programming language that is the most popular worldwide and widely used in almost every industry.\nPython’s popularity is owed to its flexibility – it is the second-best tool for every job.\nIt is a strong choice for data science and analytics, being one of the best languages for data wrangling, data visualisation, statistics, and machine learning.\n\nIt is also well-suited to web development, scientific computing, and automation."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#dependency-management",
    "href": "sessions/01-onboarding/slides.html#dependency-management",
    "title": "Python Onboarding",
    "section": "Dependency Management",
    "text": "Dependency Management\n\nDependency management refers to the process of tracking and managing all of the packages (dependencies) a project needs to run. It ensures:\n\nThe right packages are installed.\nThe correct versions are used.\nConflicts between packages are avoided.\n\nWe are using uv for dependency management."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#virtual-environments",
    "href": "sessions/01-onboarding/slides.html#virtual-environments",
    "title": "Python Onboarding",
    "section": "Virtual Environments",
    "text": "Virtual Environments\n\nVirtual environments are isolated Python environments that allow you to manage dependencies for a specific project without the state of those dependencies affecting other projects or your wider system. They help by:\n\nKeeping dependencies separate for each project.\nAvoiding version conflicts between projects.\nMaking dependency management more predictable and reproducible.\n\nVirtual environments are a part of dependency management, and we will use uv to manage both the dependencies and virtual environments."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#version-control",
    "href": "sessions/01-onboarding/slides.html#version-control",
    "title": "Python Onboarding",
    "section": "Version Control",
    "text": "Version Control\n\nVersion control is the practice of tracking and managing changes to code or files over time, allowing you to:\n\nRevert to earlier versions if needed.\nCollaborate with others on the same project easily.\nMaintain a history of changes.\n\nWe are using Git (the version control system) and GitHub (the platform for hosting our work)."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#ide",
    "href": "sessions/01-onboarding/slides.html#ide",
    "title": "Python Onboarding",
    "section": "IDE",
    "text": "IDE\n\nAn IDE (Integrated Development Environment) is fully featured software that provides everything you need to write code as conveniently as possible.\nIt typically includes a code editor, debugger, build tools, and features like syntax highlighting and code completion.\nSome common IDEs used for Python include VS Code, PyCharm, Vim, Jupyter Notebooks/JupyterLab, and Positron.\nWe will use VS Code or Jupyter Notebooks (which is not exactly an IDE but is similar)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Code Club!",
    "section": "",
    "text": "Code Club aims to support everyone at SCW in developing technical and analytical skills through interpretive dance code. We believe these skills are indispensable to the NHS today and in the future, enabling the delivery of high-quality insights through data science and advanced analytics, and the automation of day-to-day tasks with programming. We want to foster an environment that welcomes everybody, sparks ideas, and nurtures collaboration.\nThe Code Club syllabus has been designed to help people with little to no coding experience develop their skills in Python and extend their analytical skills through code. Sessions will be an hour long and held once per fortnight at 2:00 PM on Thursdays. To get an idea of what we will be covering and see if it is right for you, go to the Schedule page. We would love for you to join us!"
  }
]