---
title: "EDA"
format:
  html: default
  ipynb: default
---

We are using Australian weather data, taken from Kaggle. To download the data, click [here](data/weatherAUS.csv).
```{python}
# install necessary packages
!uv add skimpy

# import packages
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

from skimpy import skim
```

```{python}
# import the dataset
df = pd.read_csv('data/weatherAUS.csv')
```


```{python}
# use the ! prefix to interact with the terminal in a notebook
!python --version
# !python --help
```

# Setting the Scene

- What are we trying to achieve?
- How do our goals impact our analysis?
- What should we take into consideration before we write any code?

- What sort of questions might we be interested in with this dataset?

## What Our Data Can Tell Us (And What it Can't)

- How was the data collected? 
- What is it missing?
- What do the variables in our dataset actually mean, and are they a good approximation of the concepts we are interested in?

![](https://i.kym-cdn.com/entries/icons/original/000/039/191/EqR7AbhVQAAuuvA.jpg)

## Population vs Sample

- What is the difference between population data and sample data?
- Why do we care?
- How do methods/approaches differ when dealing with population data versus sample data?

## Description vs Explanation (Inference) vs Prediction

- What do these different types of analysis mean?
- Do they impact the way we structure our analysis?
- Do they impact the code we write?

# Describing Data

- What do we want to know about a dataset when we first encounter it?
- How do we get a quick overview of the data that can help us in our next steps?
- We need to get a "feel" for the data before we can really make any decisions about how to analyse it. How do we get there with a new dataset?


```{python}
# view the top five rows
df.head()
```


```{python}
# view the bottom ten rows
df.tail(10)
```


```{python}
# get the object shape (number of rows, number of columns)
df.shape
```


```{python}
# get the object length
len(df)
```


```{python}
# get all column names
df.columns
```


```{python}
# get dataframe info (column indices, non-null counts, data types)
df.info()
```


```{python}
# calculate the percentage of null values in each column
df.isnull().sum()/len(df)
```

# Wrangling Data

- Datasets are rarely perfectly clean and [tidy](https://vita.had.co.nz/papers/tidy-data.pdf). It is often necessary to make changes to our data, such as selecting a subset of columns, filtering for a range of values, and transforming how certain variables are represented.
- What "functions" might we need to carry out on our data when we are exploring it?
- What sort of transformations might be necessary here?


```{python}
# selecting a single column by name
df['Date']
```


```{python}
# there's always multiple ways to achieve something in python
df.loc[:, 'Date'] # this is good
# df.Date
```


```{python}
# selecting multiple columns (and all rows) by name
df[['Date', 'Location', 'Rainfall']]
# df.loc[:, ['Date', 'Location', 'Rainfall']]
```


```{python}
# slicing by rows
df[200:211]
```


```{python}
# find unique values
df['Location'].unique()
```


```{python}
# filtering by values
# df[df['Location'] == 'Perth']
# df[df['Rainfall'] > 0]
df[(df['Rainfall'] == 0) & (df['Location'] == 'Perth')]
```


```{python}
# convert date column to datetime
df['Date'] = pd.to_datetime(df['Date'])
```


```{python}
# convert object columns to categorical
df.apply(lambda x: x.astype('category') if x.dtype == 'object' else x)
```


```{python}
# filter observations where sunshine is NA
# (this is illustrative but should not be done without careful consideration generally)
df[df['Sunshine'].notnull()]
```

# Summarising Data


```{python}
# quick summary of numeric variables
df.describe()
```


```{python}
# a more informative summary function from the skimpy package
skim(df)
```


```{python}
# count unique values
df['Location'].nunique()
```


```{python}
# get unique values
df['Location'].unique()
```


```{python}
# calculate variable mean
np.round(df['Sunshine'].mean(), decimals=2)
```


```{python}
# calculate other summary statistics
df['Sunshine'].median()
# df['Sunshine'].sum()
```


```{python}
# calculate group means
np.round(df.groupby(by='Location')['Sunshine'].mean(), decimals=1)
```


```{python}
# group by location and count non-null sunshine values
df.groupby('Location')['Sunshine'].count()
```

# Exercises

Some of these questions are easily answered by scrolling up and finding the answer in the output of the above code, however, the goal is to find the answer using code. No one actually cares what the answer to any of these questions is, it's the process that matters!

**Remember, if you don't know the answer, it's okay to Google it (or speak to others, including me, for help)!**

1. What is the 'Sunshine' column's data type?
2. Identify all the columns that are of dtype 'object'.
3. How many of the dataframe's columns are of dtype 'object'?
4. How many of the 'Rainfall' column values are NAs?
5. Create a new dataframe which only includes the 'Date', 'Location, 'Sunshine', 'Rainfall', and 'RainTomorrow' columns.
6. Convert 'RainTomorrow' to a numeric variable, where 'Yes' = 1 and 'No' = 0.
7. What is the average amount of rainfall for each location?
8. What is the average amount of rainfall for days that it will rain tomorrow?
9. What is the average amount of sunshine in Perth when it will not rain tomorrow?
10. We want to understand the role that time plays in the dataset. Using the original dataframe, carry the following tasks and answer the corresponding questions:
    - Create columns representing the year and month from the 'Date' column. How many years of data are in the dataset?
    - Examine the distribution of the 'Sunshine' NAs over time. Is time a component in the 'Sunshine' data quality issues?
    - Calculate the average rainfall and sunshine by month. How do rainfall and sunshine vary through the year?
    - Calculate the average rainfall and sunshine by year. How have rainfall and sunshine changed over time?

# Exploring Data Using Statistics & Data Visualisation

Objectives:

- Understand how to describe data quantitatively and when different methods are appropriate
- Visualise data as a means of describing it
- Show how to visualising data is a shortcut for describing central tendency, spread, uncertainty etc.
- Introduce ideas around describing and visualising relationships between variables


```{python}
# import packages
import warnings

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

# control some deprecation warnings in seaborn
warnings.filterwarnings(
    "ignore",
    category=FutureWarning,
    module="seaborn"
)

# set figure size
plt.rcParams['figure.figsize'] = (12, 6)
```


```{python}
# subset of observations from five biggest cities
big_cities = (
    df.loc[df['Location'].isin(['Adelaide', 'Brisbane', 'Melbourne', 'Perth', 'Sydney'])]
    .copy()
)
```

## Visualising a Single Variable

- What do we want to know when we are visualising a sample taken from a single variable?
- We want to understand the value that the value tends to take, and how much it tends to deviate from its typical value.
    - The central tendency and deviation are ways to describe a sample.
    - Visualising the distribution of a variable can tell us these things (approximately), and can tell us about the shape of the data too.

### Describing a Sample

- What is the best way to describe a variable?
    - What is the average value? Or the value it is most likely to take? What is the best value to describe it in one go?
- The "central tendency" is the average or most common value that a variable takes. Mean, median, and mode are all descriptions of the central tendency.
    - Mean - Sum of values in a sample divided by the total number of observations
    - Median - The midpoint value if the sample is ordered from highest to lowest
    - Mode - The most common value in the sample
- The mean is the most common approach, but the mean, median, and mode choice are context-dependent. Other approaches exist, too, such as the geometric mean.
    - The geometric mean multiplies all values in the sample and takes the $n$th root of that multiplied value.
    - It can be useful when dealing with skewed data or data with very large ranges, and when dealing with rates, proportions etc. However it can't handle zeros or negative values.
- The mode value is generally most useful when dealing with categorical variables.


```{python}
# mode rainfall by location
big_cities.groupby('Location')['Rainfall'].agg(pd.Series.mode)
```


```{python}
# mode location
big_cities['Location'].agg(pd.Series.mode)
```


```{python}
# mode location using value counts
big_cities['Location'].value_counts().iloc[0:1]
```


```{python}
# mean rainfall by location
np.round(big_cities.groupby('Location')['Rainfall'].mean(), decimals=2)
```


```{python}
# median rainfall by location
big_cities.groupby('Location')['Rainfall'].median()
```


```{python}
# geometric mean max temperature by location
big_cities.groupby('Location')['MaxTemp'].apply(lambda x: np.exp(np.log(x).mean()))
```

- Why do the mean and median differ so much? Why would the median rainfall be zero for all five cities?
- Does this matter? How would it change our understanding of the rainfall variable?
- Visualising the distribution can tell us more!

### Comparing the Mean & Median

- We have simulated three different distributions that have slightly different shapes. and see how their mean and median values differ.

```{python}
# generate distributions
np.random.seed(123)
normal_dist = np.random.normal(10, 1, 1000)
right_skewed_dist = np.concatenate([np.random.normal(8, 2, 600), np.random.normal(14, 4, 400)])
left_skewed_dist = np.concatenate([np.random.normal(14, 2, 600), np.random.normal(8, 4, 400)])
```


```{python}
# function for calculating summary statistics and plotting distributions
def plot_averages(ax, data, title):
    mean = np.mean(data)
    median = np.median(data)
    
    sns.histplot(data, color="#d9dcd6", bins=30, ax=ax)
    ax.axvline(mean, color="#0081a7", linewidth=3, linestyle="--", label=f"Mean: {mean:.2f}")
    ax.axvline(median, color="#ef233c", linewidth=3, linestyle="--", label=f"Median: {median:.2f}")
    ax.set_title(title)
    ax.set_ylabel('')
    ax.legend()
```


```{python}
# plot distributions
fig, axes = plt.subplots(1, 3, sharey=True)

plot_averages(axes[0], normal_dist, "Normal Distribution\n(Mean ≈ Median)")
plot_averages(axes[1], right_skewed_dist, "Right-Skewed Distribution\n(Mean > Median)")
plot_averages(axes[2], left_skewed_dist, "Left-Skewed Distribution\n(Mean < Median)")

plt.suptitle("Comparison of Mean & Median Across Distributions", fontsize=16)
plt.tight_layout()
plt.show()
```

- The mean and median of the normal distribution are identical, while the two skewed distributions have slightly different means and medians.
    - The mean is larger than the media when the distribution is right-skewed, and the median is larger than the mean when it is left-skewed. 
    - When the distribution is skewed, the median value will be a better description of the central tendency, because the mean value is more sensitive to extreme values (and skewed distributions have longer tails of extreme values).
- These differences point to another important factor to consider when summarising data - the spread or deviation of the sample.
- How do we measure how a sample is spread around the central tendency?
    - Standard deviation and variance quantify spread.
    - Variance, the average squared difference between observations and the mean value, measures how spread out a sample is.
    - Standard deviation is the square root of the variance. It's easier to interpret because it's in the same units as the sample.


```{python}
# generate distributions
np.random.seed(123)
mean = 10
std_devs = [1, 2, 3]
distributions = [np.random.normal(mean, std_dev, 1000) for std_dev in std_devs]
```


```{python}
# function for calculating summary statistics and plotting distributions
def plot_spread(ax, data, std_dev, title):
    mean = np.mean(data)
    std_dev = np.std(data)

    sns.histplot(data, color="#d9dcd6", bins=30, ax=ax)
    ax.axvline(mean, color="#0081a7", linewidth=3, linestyle="--", label=f"Mean: {mean:.2f}")
    ax.axvline(mean + std_dev, color="#ee9b00", linewidth=3, linestyle="--", label=f"Mean + 1 SD: {mean + std_dev:.2f}")
    ax.axvline(mean - std_dev, color="#ee9b00", linewidth=3, linestyle="--", label=f"Mean - 1 SD: {mean - std_dev:.2f}")
    ax.set_title(f"{title}")
    ax.legend()
```


```{python}
# plot distributions
fig, axes = plt.subplots(1, 3, sharey=True, sharex=True)

for i, std_dev in enumerate(std_devs):
    plot_spread(axes[i], distributions[i], std_dev, f"Standard Deviation = {std_dev}")

plt.suptitle("Effect of Standard Deviation on Distribution Shape", fontsize=16)
plt.tight_layout()
plt.show()
```

- As standard deviation increases, the spread of values around the mean increases.
- We can compute various summary statistics that describe a sample (mean, median, standard deviation, kurtosis etc. etc.), or we can just visualise it!
- Visualising distributions is a good starting point for understanding a sample. It can quickly and easily tell you a lot about the data.


```{python}
# plot distribution of rainfall
rainfall_mean = np.mean(big_cities['Rainfall'])
rainfall_median = np.median(big_cities['Rainfall'].dropna())

sns.histplot(data=big_cities, x='Rainfall', binwidth=10, color="#d9dcd6")
plt.axvline(rainfall_mean, color="#0081a7", linestyle="--", linewidth=2, label=f"Mean: {rainfall_mean:.2f}")
plt.axvline(rainfall_median, color="#ef233c", linestyle="--", linewidth=2, label=f"Median: {rainfall_median:.2f}")

plt.title("Distribution of Rainfall in Australia's Big Cities")

plt.legend()
plt.tight_layout()
plt.show()
```


```{python}
# plot distribution of sunshine
sunshine_mean = np.mean(big_cities['Sunshine'])
sunshine_median = np.median(big_cities['Sunshine'].dropna())

sns.histplot(data=big_cities, x='Sunshine', binwidth=1, color="#d9dcd6")
plt.axvline(sunshine_mean, color="#0081a7", linestyle="--", linewidth=2, label=f"Mean: {sunshine_mean:.2f}")
plt.axvline(sunshine_median, color="#ef233c", linestyle="--", linewidth=2, label=f"Median: {sunshine_median:.2f}")

plt.title("Distribution of Sunshine in Australia's Big Cities")

plt.legend()
plt.tight_layout()
plt.show()
```

- These two plots require a little more code, but we can get most of what we want with a lot less.


```{python}
sns.histplot(data=big_cities, x='MaxTemp')
plt.tight_layout()
plt.show()
```


```{python}
sns.countplot(big_cities, x='Location', color="#d9dcd6", edgecolor='black')
plt.ylim(3000, 3500)
plt.tight_layout()
plt.show()
```


```{python}
sns.countplot(big_cities, x='RainTomorrow', color="#d9dcd6", edgecolor='black')
plt.tight_layout()
plt.show()
```


```{python}
big_cities['RainTomorrow'].value_counts()
```

## Visualising Multiple Variables

- We will often want to know how values of a given variable change based on the values of another.
- This may not indicate a relationship, but it helps us better understand our data.

```{python}

sns.barplot(big_cities, x='Location', y='Sunshine', color="#d9dcd6", edgecolor='black')
plt.tight_layout()
plt.show()

```


```{python}
sns.boxplot(big_cities, x='Location', y='MaxTemp', color="#d9dcd6")
plt.tight_layout()
plt.show()
```


```{python}
sns.boxplot(data=big_cities, x='RainTomorrow', y='Humidity3pm', color="#d9dcd6")
plt.tight_layout()
plt.show()
```


```{python}
sns.kdeplot(data=big_cities, x='Humidity3pm', hue='RainTomorrow')
plt.tight_layout()
plt.show()
```


```{python}
(
    big_cities
    # convert date to datetime
    .assign(Date=pd.to_datetime(big_cities['Date']))
    # create year-month column
    .assign(Year_Month=lambda x: x['Date'].dt.to_period('M'))
    # group by year-month and calculate sum of rainfall
    .groupby('Year_Month')['Rainfall'].sum()
    # convert year-month index back to column in dataframe
    .reset_index()
    # create year-month timestamp for plotting
    .assign(Year_Month=lambda x: x['Year_Month'].dt.to_timestamp()) 
    # pass df object to seaborn lineplot
    .pipe(lambda df: sns.lineplot(data=df, x='Year_Month', y='Rainfall', linewidth=2))
)

plt.tight_layout()
plt.show()
```


```{python}
(
    big_cities
    # convert date to datetime object
    .assign(Date=pd.to_datetime(big_cities['Date']))
    # set date column as index
    .set_index('Date')
    # resample by month-end for monthly aggregations
    .resample('ME')
    # calculate mean sunshine per month
    .agg({'Sunshine': 'mean'})
    # convert month index back to column in dataframe
    .reset_index()
    # pass df object to seaborn lineplot
    .pipe(lambda df: sns.lineplot(data=df, x='Date', y='Sunshine', color="#1f77b4", linewidth=2))
)

plt.tight_layout()
plt.show()

```


```{python}
fig, axes = plt.subplots(1, 2)

(
    big_cities
    .assign(Date=pd.to_datetime(big_cities['Date']))
    .assign(Month=lambda x: x['Date'].dt.month)
    .groupby('Month')['Rainfall'].mean()
    .reset_index()
    .pipe(lambda df: sns.lineplot(data=df, x='Month', y='Rainfall', color="#1f77b4", linewidth=2, ax=axes[0]))
)

(
    big_cities
    .assign(Date=pd.to_datetime(big_cities['Date']))
    .assign(Month=lambda x: x['Date'].dt.month) 
    .groupby('Month')['Sunshine'].mean() 
    .reset_index()
    .pipe(lambda df: sns.lineplot(data=df, x='Month', y='Sunshine', color="#ff7f0e", linewidth=2, ax=axes[1]))
)

xticks = range(1, 13)
xticklabels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
for ax in axes:
    ax.set_xticks(xticks)  # Set ticks
    ax.set_xticklabels(xticklabels, rotation=45)
    ax.set_xlabel('')
    ax.set_ylabel('')
axes[0].set_title('Average Rainfall by Month', fontsize=16)
axes[1].set_title('Average Sunshine by Month', fontsize=16)

plt.tight_layout()
plt.show()
```

## Visualising Relationships

- Just comparing how the values of variables change relative to changes in another variable is insufficient for assessing a "relationship" between two variables.
- Visualising distributions tell us about the central tendency spread and shape of a sample, but we are usually interested in the relationship between variables.
- Visualising how one variable's values differ by another's value is a good starting point for understanding their relationship.
- Two ways of understanding the relationships between variables (in a descriptive sense), are covariance and correlation.
    - Covariance describes how much two variables change together.
    - Correlation can be understood as like a standardised version of covariance.


```{python}
np.random.seed(123)

x = np.random.uniform(-1, 1, 1000)
y1 = np.random.uniform(-1, 1, 1000)
y2 = 0.5 * x + np.random.normal(0, 0.5, 1000)
y3 = 0.9 * x + np.random.normal(0, 0.1, 1000)

datasets = {
    "No Correlation": (x, y1),
    "Moderate Correlation": (x, y2),
    "Strong Correlation": (x, y3),
}
```


```{python}
fig, axes = plt.subplots(1, 3, sharex=True, sharey=True)

for ax, (title, (x, y)) in zip(axes, datasets.items()):
    sns.scatterplot(x=x, y=y, ax=ax, alpha=0.7)
    correlation = np.corrcoef(x, y)[0, 1]
    ax.text(0.05, 0.95, f"r: {correlation:.2f}", transform=ax.transAxes, fontsize=12, ha="left", va="top")
    ax.set_title(title)
    ax.set_xlabel("X")
    ax.set_ylabel("Y")

plt.suptitle("Visualizing Correlation", fontsize=16)
plt.tight_layout()
plt.show()
```


```{python}
x1 = np.random.uniform(0, 1, 1000)
y1 = np.random.uniform(0, 1, 1000)
x2 = np.random.uniform(0, 10, 1000)
y2 = 2 * x2 + np.random.normal(0, 5, 1000)
x3 = np.random.uniform(0, 1, 1000)
y3 = 0.9 * x3 + np.random.normal(0, 0.1, 1000)
x4 = np.random.uniform(0, 10, 1000)
y4 = 2 * x4 + np.random.normal(0, 1, 1000)

datasets = {
    "Low Covariance, Low Correlation": (x1, y1),
    "High Covariance, Low Correlation": (x2, y2),
    "Low Covariance, High Correlation": (x3, y3),
    "High Covariance, High Correlation": (x4, y4),
}
```


```{python}
fig, axes = plt.subplots(2, 2, figsize=(12, 10), sharex=False, sharey=False)

for ax, (title, (x, y)) in zip(axes.flat, datasets.items()):
    sns.scatterplot(x=x, y=y, ax=ax, alpha=0.7)
    covariance = np.cov(x, y)[0, 1]
    correlation = np.corrcoef(x, y)[0, 1]
    ax.text(0.05, 0.95, f"Cov: {covariance:.2f}\nCorr: {correlation:.2f}",
            transform=ax.transAxes, fontsize=12, ha="left", va="top",
            bbox=dict(facecolor='white', boxstyle='round,pad=0.3'))
    ax.set_title(title, fontsize=14)
    ax.set_xlabel("X", fontsize=12)
    ax.set_ylabel("Y", fontsize=12)

plt.suptitle("Understanding Covariance and Correlation", fontsize=16)
plt.tight_layout()
plt.show()
```


```{python}
humidity = big_cities.dropna(subset=['Humidity9am', 'Humidity3pm'])

fig, ax = plt.subplots()

sns.scatterplot(data=humidity, x='Humidity9am', y='Humidity3pm', ax=ax, alpha=0.7)

covariance = np.cov(humidity['Humidity9am'], humidity['Humidity3pm'])[0, 1]
correlation = np.corrcoef(humidity['Humidity9am'], humidity['Humidity3pm'])[0, 1]

ax.text(0.05, 0.95, f"Cov: {covariance:.2f}\nCorr: {correlation:.2f}",
        transform=ax.transAxes, fontsize=12, ha="left", va="top",
        bbox=dict(facecolor='white', boxstyle='round,pad=0.6'))

plt.tight_layout()
plt.show()
```


```{python}
temps = big_cities.dropna(subset=['Temp9am', 'Temp3pm'])

fig, ax = plt.subplots()

sns.scatterplot(data=temps, x='Temp9am', y='Temp3pm', ax=ax, alpha=0.7)

covariance = np.cov(temps['Temp9am'], temps['Temp3pm'])[0, 1]
correlation = np.corrcoef(temps['Temp9am'], temps['Temp3pm'])[0, 1]

ax.text(0.05, 0.95, f"Cov: {covariance:.2f}\nCorr: {correlation:.2f}",
        transform=ax.transAxes, fontsize=12, ha="left", va="top",
        bbox=dict(facecolor='white', boxstyle='round,pad=0.6'))

plt.tight_layout()
plt.show()
```


```{python}
(
    # create contingency table of rain today vs rain tomorrow
    pd.crosstab(big_cities['RainToday'], big_cities['RainTomorrow'])
    # divide each row (count) by the sum of the row, to produce proportions 
    .div(pd.crosstab(big_cities['RainToday'], big_cities['RainTomorrow']).sum(axis=1), axis=0)
    # pass df object to seaborn and create a heatmap
    .pipe(lambda df: sns.heatmap(df, annot=True, fmt='.2f', cmap="Blues", cbar=True))
)

plt.tight_layout()
plt.show()
```

- What these don't tell us are how much of the changes in a variable relative to another are caused by chance versus how much is "meaningful".
- To do this, we first have to have some sense of uncertainty, and we need to be able to understand how much of the variance in the outcome (the variable which changes relative to changes in the other variable) can be explained by variance in the predictor.
- Correlation =/= Causation
    - The amount of covariance/correlation doesn't necessarily tell us that there is or is not a meaningful relationship between two variables.
    - But the opposite can be true too (Causation =/= Correlation)